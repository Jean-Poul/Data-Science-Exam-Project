{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9befe0f9-d6d0-43f6-88a6-71de1cdffd9c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#  Data Science - Exam Project\n",
    "\n",
    "## The Medias Influence On The Stock Market\n",
    "\n",
    "### *Hypothesis*: Can trends on the stock markets be predicted, by analyzing publicity in news outlets?\n",
    "### *Null hypothesis*: There is no correlation between news outlets and trends on the stock markets?\n",
    "\n",
    "\n",
    "### Group Members\n",
    "- Allan Simonsen, cph-as484@cphbusiness.dk\n",
    "- Jean-Poul Leth-MÃ¸ller, cph-jl360@cphbusiness.dk\n",
    "- Nina Lisakowski, cph-nl163@cphbusiness.dk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40450f26",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "* [1. Environment Setup](#chapter1)\n",
    "* [2. Import and cleaning of data](#chapter2)\n",
    "\n",
    "    * [2.1. Ticker Dataset](#chapter2.1)\n",
    "        * [2.1.1 Describing Dataset](#chapter2.1.1)\n",
    "        * [2.1.2 Initial cleaning of Dataset](#chapter2.1.2)\n",
    "        * [2.1.3 Exploring & visualizing the data](#chapter2.1.3)\n",
    "            * [2.1.3.1 Correlation & Heatmap](#chapter2.1.3.1)\n",
    "            * [2.1.3.2 Pairplots](#chapter2.1.3.2)\n",
    "            * [2.1.3.3 Histograms & Boxplots](#chapter2.1.3.3)\n",
    "            * [2.1.3.4 Exploring volume and close columns](#chapter2.1.3.4)\n",
    "            * [2.1.3.5 Exploring volume and close columns](#chapter2.1.3.5)\n",
    "        * [2.1.4 Preperation for machine learning](#chapter2.1.4)\n",
    "        \n",
    "     * [2.2. News Dataset](#chapter2.2)\n",
    "        * [2.2.1 Describing Dataset](#chapter2.2.1)\n",
    "        * [2.2.2 Initial cleaning of Dataset](#chapter2.2.2)\n",
    "        * [2.2.3 Exploring & visualizing the data](#chapter2.2.3)\n",
    "            * [2.2.3.1 Date column](#chapter2.2.3.1)\n",
    "            * [2.2.3.2 Stock Column](#chapter2.2.3.2)\n",
    "            * [2.2.3.3 Nvda Stock](#chapter2.2.3.3)\n",
    "        * [2.2.4 Preperation for machine learning](#chapter2.2.4)\n",
    "            * [2.2.4.1 PCA](#chapter2.2.4.1)\n",
    "            \n",
    "     * [2.3. Combining Dataset](#chapter2.3)\n",
    "         * [2.3.1 First Combined Dataset](#chapter2.3.1)\n",
    "         * [2.3.2 Second Combined Dataset](#chapter2.3.2)\n",
    "         * [2.3.3 Third Combined Dataset](#chapter2.3.3)\n",
    "         \n",
    "* [3. Machine Learning](#chapter3)\n",
    "\n",
    "    * [3.1. Dataset preperation for machine learning](#chapter3.1)\n",
    "        * [3.1.1 Dataset one preperation](#chapter3.1.1)\n",
    "            * [3.1.1.1 Dataset Isolation in numpy](#chapter3.1.1.1)\n",
    "        * [3.1.2 Dataset two preperation](#chapter3.1.2)\n",
    "            * [3.1.2.1 Dataset Isolation in numpy](#chapter3.1.2.1)\n",
    "        * [3.1.3 Dataset three preperation](#chapter3.1.3)\n",
    "            * [3.1.3.1 Dataset Isolation in numpy](#chapter3.1.3.1)\n",
    "    \n",
    "    * [3.2. K-means](#chapter3.2)\n",
    "        * [3.2.1 Theory](#chapter3.2.1)\n",
    "        * [3.2.2 Machine Learning Dataset One](#chapter3.2.2)\n",
    "            * [3.2.2.1 Training the model](#chapter3.2.2.1)\n",
    "            * [3.2.2.2 Evaluating the model](#chapter3.2.2.2)\n",
    "            * [3.2.2.3 Estimating the errors in prediction](#chapter3.2.2.3)\n",
    "            * [3.2.2.4 Second run of k-means with a more optimal value of k](#chapter3.2.2.4)\n",
    "            * [3.2.2.5 Conclusion](#chapter3.2.2.5)\n",
    "    \n",
    "    * [3.3. K-Nearest Neighbors](#chapter3.3)\n",
    "        * [3.3.1 Theory](#chapter3.3.1)\n",
    "        * [3.3.2 Machine Learning Dataset One](#chapter3.3.2)\n",
    "            * [3.3.2.1 Training the model](#chapter3.3.2.1)\n",
    "            * [3.3.2.2 Evaluating the model](#chapter3.3.2.2)\n",
    "            * [3.3.2.3 Estimating the errors in prediction](#chapter3.3.2.3)\n",
    "            * [3.3.2.4 Visualizing data](#chapter3.3.2.4)\n",
    "            * [3.3.2.5 Validating with validation set](#chapter3.3.2.5)\n",
    "        * [3.3.3 Machine Learning Dataset two](#chapter3.3.3)\n",
    "            * [3.3.3.1 Training the model](#chapter3.3.3.1)\n",
    "            * [3.3.3.2 Evaluating the model](#chapter3.3.3.2)\n",
    "            * [3.3.3.3 Estimating the errors in prediction](#chapter3.3.3.3)\n",
    "            * [3.3.3.4 Visualizing data](#chapter3.3.3.4)\n",
    "            * [3.3.3.5 Validating with validation set](#chapter3.3.3.5)\n",
    "        * [3.3.4 Machine Learning Dataset Three](#chapter3.3.4)\n",
    "            * [3.3.4.1 Training the model](#chapter3.3.4.1)\n",
    "            * [3.3.4.2 Evaluating the model](#chapter3.3.4.2)\n",
    "            * [3.3.4.3 Estimating the errors in prediction](#chapter3.3.4.3)\n",
    "            * [3.3.4.4 Visualizing data](#chapter3.3.4.4)\n",
    "            * [3.3.4.5 Validating with validation set](#chapter3.3.4.5)\n",
    "        * [3.3.5 Summary](#chapter3.3.5)\n",
    "\n",
    "    * [3.4. Guassian Naive Bayes](#chapter3.4)\n",
    "        * [3.4.1 Theory](#chapter3.4.1)\n",
    "        * [3.4.2 Machine Learning Dataset One](#chapter3.4.2)\n",
    "            * [3.4.2.1 Training the model](#chapter3.4.2.1)\n",
    "            * [3.4.2.2 Evaluating the model](#chapter3.4.2.2)\n",
    "            * [3.4.2.3 Estimating the errors in prediction](#chapter3.4.2.3)\n",
    "            * [3.4.2.4 Visualizing data](#chapter3.4.2.4)\n",
    "            * [3.4.2.5 Validating with validation set](#chapter3.4.2.5)\n",
    "        * [3.4.3 Machine Learning Dataset two](#chapter3.4.3)\n",
    "            * [3.4.3.1 Training the model](#chapter3.4.3.1)\n",
    "            * [3.4.3.2 Evaluating the model](#chapter3.4.3.2)\n",
    "            * [3.4.3.3 Estimating the errors in prediction](#chapter3.4.3.3)\n",
    "            * [3.4.3.4 Visualizing data](#chapter3.4.3.4)\n",
    "            * [3.4.3.5 Validating with validation set](#chapter3.4.3.5)\n",
    "        * [3.4.4 Machine Learning Dataset Three](#chapter3.4.4)\n",
    "            * [3.4.4.1 Training the model](#chapter3.4.4.1)\n",
    "            * [3.4.4.2 Evaluating the model](#chapter3.4.4.2)\n",
    "            * [3.4.4.3 Estimating the errors in prediction](#chapter3.4.4.3)\n",
    "            * [3.4.4.4 Visualizing data](#chapter3.4.4.4)\n",
    "            * [3.4.4.5 Validating with validation set](#chapter3.4.4.5)\n",
    "        * [3.4.5 Summary](#chapter3.4.5)\n",
    "    * [3.5. Decision Tree](#chapter3.5)\n",
    "        * [3.5.1 Theory](#chapter3.5.1)\n",
    "        * [3.5.2 Machine Learning Dataset One](#chapter3.5.2)\n",
    "            * [3.5.2.1 Training the model](#chapter3.5.2.1)\n",
    "            * [3.5.2.2 Evaluating the model](#chapter3.5.2.2)\n",
    "            * [3.5.2.3 Estimating the errors in prediction](#chapter3.5.2.3)\n",
    "            * [3.5.2.4 Visualizing data](#chapter3.5.2.4)\n",
    "            * [3.5.2.5 Validating with validation set](#chapter3.5.2.5)\n",
    "        * [3.5.3 Machine Learning Dataset two](#chapter3.5.3)\n",
    "            * [3.5.3.1 Training the model](#chapter3.5.3.1)\n",
    "            * [3.5.3.2 Evaluating the model](#chapter3.5.3.2)\n",
    "            * [3.5.3.3 Estimating the errors in prediction](#chapter3.5.2.3)\n",
    "            * [3.5.3.4 Visualizing data](#chapter3.5.3.4)\n",
    "            * [3.5.3.5 Validating with validation set](#chapter3.5.3.5)\n",
    "        * [3.5.4 Machine Learning Dataset Three](#chapter3.5.4)\n",
    "            * [3.5.4.1 Training the model](#chapter3.5.4.1)\n",
    "            * [3.5.4.2 Evaluating the model](#chapter3.5.4.2)\n",
    "            * [3.5.4.3 Estimating the errors in prediction](#chapter3.5.4.3)\n",
    "            * [3.5.4.4 Visualizing data](#chapter3.5.4.4)\n",
    "            * [3.5.4.5 Validating with validation set](#chapter3.5.4.5)\n",
    "        * [3.5.5 Summary](#chapter3.5.5)\n",
    "    * [3.6 Summary](#chapter3.6)\n",
    "\n",
    "* [4. Visualization](#chapter4)\n",
    "* [5. Project Conclusion](#chapter5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcdd659-1b40-42a8-998e-d6eb56112d22",
   "metadata": {},
   "source": [
    "## 1. Environment Setup <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99ae392e-0ad5-49c2-8a94-e93b9381032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas -used for structuring data\n",
    "import pandas as pd\n",
    "\n",
    "# Pandas Profiling tool\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# Pandas Scatter matrix function\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Numpy - Multi dimensional datastructure tool\n",
    "import numpy as np\n",
    "\n",
    "# Seaborn & matplotlib - used for plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize']=20,10\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Text vectorization import\n",
    "import spacy\n",
    "\n",
    "# For saving and loading the model\n",
    "import joblib\n",
    "\n",
    "# For making a webserver\n",
    "import pickle\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "\n",
    "# PCA for dimentionality reduction\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Machine learning preparation and training methods\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "\n",
    "# Machine learning algorithms\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "\n",
    "# Measuring metrics for ML\n",
    "from sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, f1_score, mean_squared_error, silhouette_score, confusion_matrix\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# Finance tool\n",
    "import yfinance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74b4882-8393-48ec-8ab1-077bb48847ea",
   "metadata": {},
   "source": [
    "## 2. Import and cleaning of data <a class=\"anchor\" id=\"chapter2\"></a>\n",
    "This project contains 2 datasets.  \n",
    "- **Ticker dataset**  \n",
    "This dataset contains information about a specific stock. logging every day its opening price,closing price and volume.  \n",
    "[Link to dataset](https://www.kaggle.com/datasets/paultimothymooney/stock-market-data)  \n",
    "\n",
    "- **News media dataset**  \n",
    "This dataset contains multiple thousands of headlines spaning over a decade. Row consist of headline, date of article and ticker(the stock that is related)\n",
    "[Link to dataset](https://www.kaggle.com/datasets/miguelaenlle/massive-stock-news-analysis-db-for-nlpbacktests)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a17976-a946-4144-b857-492c3d9f685a",
   "metadata": {},
   "source": [
    "### 2.1 Ticker dataset <a class=\"anchor\" id=\"chapter2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82bb8fbb-400e-412d-a59a-cebaab2926ed",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/stock_market_data/nasdaq/csv/NVDA.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_82736/2333708169.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mticker_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../data/stock_market_data/nasdaq/csv/NVDA.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/stock_market_data/nasdaq/csv/NVDA.csv'"
     ]
    }
   ],
   "source": [
    "ticker_df = pd.read_csv('../data/stock_market_data/nasdaq/csv/NVDA.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce42c81-a7b0-4f79-9aad-0467ab751b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c7cb9d-3d45-4108-bc03-7085f47ed634",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7b988",
   "metadata": {},
   "source": [
    "#### 2.1.1 Describing dataset <a class=\"anchor\" id=\"chapter2.1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9594dc41-9b0d-403f-9bd3-337ad82f6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa29173-f011-476d-a2bf-3b79f41b9e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd66fc7-f320-4907-b8c8-0dab805d80bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d617eeda",
   "metadata": {},
   "source": [
    "Checking for null values in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164ea7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598729c0-a634-4af3-a2db-7eefe677ca37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec1fc11",
   "metadata": {},
   "source": [
    "#### 2.1.2 Initial Cleaning of dataset <a class=\"anchor\" id=\"chapter2.1.2\"></a>\n",
    "Now we want to convert the Date object to be able to match values to this converted string and use it with pandas datetime ticker_df['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9446e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = pd.to_datetime(ticker_df['Date'].str[:], format=\"%d-%m-%Y\", errors='coerce')\n",
    "ticker_df.insert(1, 'date_time', values)\n",
    "ticker_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a16f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ticker_df = ticker_df[~(ticker_df['date_time'] < '2010-08-01')]\n",
    "sorted_ticker_df = sorted_ticker_df.sort_values(by = 'date_time')\n",
    "sorted_ticker_df.reset_index(drop=True, inplace=True)\n",
    "sorted_ticker_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0808f8e8",
   "metadata": {},
   "source": [
    "#### 2.1.3 Exploring and visualizing the data <a class=\"anchor\" id=\"chapter2.1.3\"></a>\n",
    "Now let us explore and play with the dataset to gain insights after we have cleaned the data.  \n",
    "To start of with let us see if we can find any correlation in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78889b4b",
   "metadata": {},
   "source": [
    "##### 2.1.3.1 Correlation & Heatmap <a class=\"anchor\" id=\"chapter2.1.3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c70496",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = sorted_ticker_df.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3975f6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking for correlations from our corr_matrix\n",
    "corr_matrix[\"Close\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let us visualize our matrix\n",
    "sns.heatmap(corr_matrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983fad5c",
   "metadata": {},
   "source": [
    "##### 2.1.3.2 Pairplots <a class=\"anchor\" id=\"chapter2.1.3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8988f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['Low', 'Open', 'Volume', 'High', 'Close', 'Adjusted Close']\n",
    "scatter_matrix(sorted_ticker_df[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb04135",
   "metadata": {},
   "source": [
    "We then visualize this further by making a pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93950ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(sorted_ticker_df, hue='Close')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f8c46e",
   "metadata": {},
   "source": [
    "ticker_df : 'Low', 'Open', 'Volume', 'High', 'Close', 'Adjusted Close'  \n",
    "Visualizing all the features by using scatterplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6092ef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(sorted_ticker_df, x_vars=['Low', 'Open', 'Volume', 'High', 'Close'], y_vars='Close', height=5, aspect=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005e81e3",
   "metadata": {},
   "source": [
    "##### 2.1.3.3 Histograms and boxplots <a class=\"anchor\" id=\"chapter2.1.3.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ticker_df.hist(bins=50, figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f6fe2f",
   "metadata": {},
   "source": [
    "By looking at the plots we can see that Low, Open and high seem to have a linear correlation with the closing price.  \n",
    "Let us search for outliers and see how the data is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156c05f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"Volume\", y=\"Close\", data=sorted_ticker_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a33746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw box-whisker plots\n",
    "sorted_ticker_df.plot(kind='box', subplots=True, layout=(3,3), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece8496",
   "metadata": {},
   "source": [
    "##### 2.1.3.4 Exploring Volume and close columns <a class=\"anchor\" id=\"chapter2.1.3.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d0e0eb",
   "metadata": {},
   "source": [
    "We are trying to analyze whether news has an effect on price and volume.  \n",
    "Therefor our focus will be on exploring price and volume further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267df98",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['Volume', 'Close']\n",
    "scatter_matrix(sorted_ticker_df[attributes], figsize=(12, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45befd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=sorted_ticker_df[\"Volume\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000b710e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sorted_ticker_df['Volume'],  label='Volume')  \n",
    "#sns.distplot(ticker_df['Volume'],  label='Volume', norm_hist=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f3fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(y=sorted_ticker_df[\"Close\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031373e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(sorted_ticker_df['Close'],  label='Close')  \n",
    "#sns.distplot(ticker_df['Close'],  label='Close', norm_hist=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f49f33",
   "metadata": {},
   "source": [
    "Let us get a full picture of all the moves the stock has done over time by looking at the closing price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8f4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dateindex_ticker_df=sorted_ticker_df\n",
    "sorted_dateindex_ticker_df.index=sorted_ticker_df['date_time']\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(sorted_dateindex_ticker_df[\"Close\"], label='Close Price history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8677fece",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(sorted_dateindex_ticker_df[\"Volume\"], label='Volume history')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d397c53",
   "metadata": {},
   "source": [
    "##### 2.1.3.5 Exploring Volume and close columns <a class=\"anchor\" id=\"chapter2.1.3.5\"></a>\n",
    "We then zoom in to look at the last three months to potentially find the current trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b05a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_months_open = sorted_dateindex_ticker_df.iloc[-60:]\n",
    "three_months_open['Close'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ada9098",
   "metadata": {},
   "source": [
    "Viewing open vs close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8d3e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.plot(three_months_open.index, three_months_open['Open'], '--', color='r', label='Open')\n",
    "ax.plot(three_months_open.index, three_months_open['Close'], '-', color='g', label='Close')\n",
    "ax.legend()\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bdfd85",
   "metadata": {},
   "source": [
    "Looking for outliers in the closing price the past three months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c1f60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_months_open['Close'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6092bcfb",
   "metadata": {},
   "source": [
    "Looking at the volume the past three months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155d0740",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(three_months_open[\"Volume\"], label='Volume history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469dd9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_months_open['Volume'].plot(kind='box')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd229502",
   "metadata": {},
   "source": [
    "Looking for a relationship between closing price and volume over the past three months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaf77d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_months_open.plot(kind=\"scatter\", x=\"Close\", y=\"Volume\", alpha=1,\n",
    "    s=three_months_open[\"Close\"], label=\"Close vs Volume\", figsize=(10,7),\n",
    "    c=\"Volume\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "    sharex=False)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac0989e",
   "metadata": {},
   "source": [
    "#### 2.1.4 Preperation for machine learning <a class=\"anchor\" id=\"chapter2.1.4\"></a>\n",
    "We want to add a column to be able to see if our stock of interest has gone up or down on the given day.  \n",
    "First we convert this boolean values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8328b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ticker_df.reset_index(drop=True, inplace=True)\n",
    "sorted_ticker_df['Gain or Loss'] = sorted_ticker_df['Close'] > sorted_ticker_df['Open']\n",
    "print(sorted_ticker_df.shape)\n",
    "sorted_ticker_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9764a74",
   "metadata": {},
   "source": [
    "Hereafter we convert this to binary values in form of 1 or 0 to be able to use this when training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454bbe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ticker_df['Gain or Loss'] = sorted_ticker_df['Gain or Loss'].astype(int)\n",
    "\n",
    "sorted_ticker_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891d8332",
   "metadata": {},
   "source": [
    "We will now investigate which categories the target column contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a670f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ticker_df['Gain or Loss'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a87e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.countplot(x='Gain or Loss', data=sorted_ticker_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(16,9))\n",
    "\n",
    "sns.boxplot(x='Gain or Loss', y='Open', data=sorted_ticker_df, orient='v', ax=axes[0,0])\n",
    "sns.boxplot(x='Gain or Loss', y='Close', data=sorted_ticker_df, orient='v', ax=axes[0,1])\n",
    "sns.boxplot(x='Gain or Loss', y='High', data=sorted_ticker_df, orient='v', ax=axes[1,0])\n",
    "sns.boxplot(x='Gain or Loss', y='Volume', data=sorted_ticker_df, orient='v', ax=axes[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e2a4e3-3a26-4a0d-8d3a-0956d5831ae1",
   "metadata": {},
   "source": [
    "### 2.2 News data set <a class=\"anchor\" id=\"chapter2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986daf69-8642-4749-835a-f70bc582bf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = pd.read_csv('../data/news/analyst_ratings_processed.csv', delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6dd5a1-d445-44bc-a1f6-f74a938ad7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cfb57d-91e7-4588-aa41-af3cf6bcb1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f8f5ef",
   "metadata": {},
   "source": [
    "#### 2.2.1 Describing data set <a class=\"anchor\" id=\"chapter2.2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5ca839",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9c6c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d63fd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf63933f",
   "metadata": {},
   "source": [
    "#### 2.2.2 Initial Cleaning of dataset <a class=\"anchor\" id=\"chapter2.2.2\"></a>\n",
    "Our todo list for cleaning is as following:\n",
    "1. Remove the unnamed column, as this column represent some kind of index, however since there are null values, its pretty useless\n",
    "2. Remove rows that contain any kind of null value. Since we have 1m+ data rows we wont worry about a few thousands being dropped. since there is no value in using any average tool or other techniques to fill out these rows.\n",
    "3. Converting dates into pandas datatime objects in a new column called date_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5119e54",
   "metadata": {},
   "source": [
    "Removing Unnamed column (index)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769a6a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_df.iloc[:,1:]\n",
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1750705e",
   "metadata": {},
   "source": [
    "Removing rows that contain null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049f785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.dropna(inplace=True)\n",
    "news_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a94eb6",
   "metadata": {},
   "source": [
    "Converting date into pandas datetime and putting into date_time column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.insert(2, 'date_time', pd.to_datetime(news_df['date'].str[:19], format=\"%Y-%m-%d\", errors='coerce').dt.date)\n",
    "news_df['date_time'] = pd.to_datetime(news_df['date_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deeb5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['date_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafb674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b96e20d",
   "metadata": {},
   "source": [
    "#### 2.2.3 Exploring and visualizing the data <a class=\"anchor\" id=\"chapter2.2.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c39616e",
   "metadata": {},
   "source": [
    "##### 2.2.3.1 Date Column <a class=\"anchor\" id=\"chapter2.2.3.1\"></a>\n",
    "We want to investigate the date column to see when our news data exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1eef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5286622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_news_df = news_df.sort_values(by = 'date_time')\n",
    "sorted_news_df['combined'] = sorted_news_df['date_time'].dt.month.astype(str).str.zfill(2) + '-' +  sorted_news_df['date_time'].dt.year.astype(str).str[:4]\n",
    "sorted_news_df['combined'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0237a11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "chart = sns.countplot(x='combined', data=sorted_news_df)\n",
    "chart.tick_params(axis = 'x', rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81deeef",
   "metadata": {},
   "source": [
    "We can see that there are very few news stories from before 08-2020. therefore we will remove this data, this account for around 45k rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15ca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df = news_df[~(news_df['date_time'] < '2010-08-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8797be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_news_df = news_df.sort_values(by = 'date_time')\n",
    "sorted_news_df['combined'] = sorted_news_df['date_time'].dt.month.astype(str).str.zfill(2) + '-' +  sorted_news_df['date_time'].dt.year.astype(str).str[:4]\n",
    "sorted_news_df['combined'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df0fcea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "chart = sns.countplot(x='combined', data=sorted_news_df)\n",
    "chart.tick_params(axis = 'x', rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13480e02",
   "metadata": {},
   "source": [
    "#### 2.2.3.2 Stock Column <a class=\"anchor\" id=\"chapter2.2.3.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e884f003",
   "metadata": {},
   "source": [
    "We want to investigate how many news stories that are related to each stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = news_df[\"stock\"].value_counts()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a1921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(16,5)})\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.boxplot(x=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd433fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_interesting_stocks = df[~(df.iloc[:] <= 2000)]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea9f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_interesting_stocks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65000166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "most_interesting_stocks.head(41)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7cfb6c",
   "metadata": {},
   "source": [
    "##### 2.2.3.3 NVDA Stock <a class=\"anchor\" id=\"chapter2.2.3.3\"></a>\n",
    "We are going to isolate and look at the NVDA stock. therefore we would like to visualize any trends and see the distribution of stories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da1ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_news_df['combined'] = sorted_news_df['date_time'].dt.month.astype(str).str.zfill(2) + '-' +  sorted_news_df['date_time'].dt.year.astype(str).str[:4]\n",
    "nvda_df = sorted_news_df.loc[sorted_news_df['stock'] == 'NVDA']\n",
    "nvda_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "nvda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d784091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "chart = sns.countplot(x='combined', data=nvda_df)\n",
    "chart.tick_params(axis = 'x', rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd506e9",
   "metadata": {},
   "source": [
    "We see that there is a higher concentration in the latter years. however we would like to see how the data looks on a day to day basis. So we are going to isolate the last three years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd47815",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvda_df = nvda_df[~(nvda_df['date_time'] < '2019-12-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185dfb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvda_df['combined'] = nvda_df['date_time'].dt.day.astype(str).str.zfill(2) + '-' +nvda_df['date_time'].dt.month.astype(str).str.zfill(2) + '-' +  sorted_news_df['date_time'].dt.year.astype(str).str[:4]\n",
    "nvda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee4799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc = {'figure.figsize':(15,8)})\n",
    "chart = sns.countplot(x='combined', data=nvda_df)\n",
    "chart.tick_params(axis = 'x', rotation = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32a944d",
   "metadata": {},
   "source": [
    "Usually there are few stories(1-2) a day. However we also see days with massive spikes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17ba08d",
   "metadata": {},
   "source": [
    "#### 2.2.4 Preperation for machine learning; With vectorization <a class=\"anchor\" id=\"chapter2.2.4\"></a>\n",
    "In preperation for machine learning we will vectorize the headlines for each news article."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fdda08",
   "metadata": {},
   "source": [
    "First install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63234771",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip setuptools wheel\n",
    "!pip install -U 'spacy[cuda113]'\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b7b0d",
   "metadata": {},
   "source": [
    "We set up the pipeline from spacy we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1d3eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "pipeline = [\"tok2vec\", \"tagger\", \"parser\", \"ner\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a2cbf1",
   "metadata": {},
   "source": [
    "For peformance reasons, we will only create vectors for the NVDA stock, so we first isolate those news stories related to that stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df35aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvda_df = news_df.loc[news_df['stock'] == 'NVDA']\n",
    "nvda_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "nvda_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f302d6",
   "metadata": {},
   "source": [
    "Afterwards we use spacy to pipe each headline from the NVDA set and find the tokens vector. The vector is appended to an empty list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e28f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs = nvda_df['title']\n",
    "\n",
    "vec_tokens = []\n",
    "for token in nlp.pipe(docs):\n",
    "    vec_tokens.append(token.vector)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1296fa59",
   "metadata": {},
   "source": [
    "There are the same number of vectors as there are headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874a1f16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(vec_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ccbd7f",
   "metadata": {},
   "source": [
    "We add the vectors to the NVDA-dataframe in a vector column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88afbb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvda_df.insert(1, 'vectors', vec_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33332d",
   "metadata": {},
   "source": [
    "Below the result for the dataframe can be seen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9828422f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvda_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192ce9f6",
   "metadata": {},
   "source": [
    "Isolating the vector values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e6e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = nvda_df.iloc[:, 1:2].values\n",
    "vector_array = np.empty((0, 300), float)\n",
    "for row in X:\n",
    "    newrow = row[0].tolist()\n",
    "    vector_array = np.append(vector_array, np.array([newrow]), axis=0)\n",
    "vector_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b415d88a",
   "metadata": {},
   "source": [
    "Using kmeans label the titles into two clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdbaf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(vector_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf7ed78",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c62f4dd",
   "metadata": {},
   "source": [
    "##### 2.2.4.1 PCA  <a class=\"anchor\" id=\"chapter2.2.4.1\"></a>\n",
    "First we find the optimail amount of features that pca recommend.\n",
    "After that we completely ignore this recommendation and reduce down to 2 dimentions so we can plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dedb9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whats optimal?\n",
    "pca_opt = PCA(0.95)\n",
    "pca_array = pca_opt.fit_transform(vector_array)\n",
    "pca_opt.n_components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadf81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce to two dimentional\n",
    "pca = PCA(n_components=2)\n",
    "pca_array = pca.fit_transform(vector_array)\n",
    "pca_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743990ed",
   "metadata": {},
   "source": [
    "Plotting the two dimentional vectors with the labels from K-Means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e8bc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_array[:,0], pca_array[:,1], c=kmeans.labels_, marker=\"o\", picker=True, cmap=\"cool\")\n",
    "#plt.title(f'Estimated number of clusters = {n_clusters_}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2824b8",
   "metadata": {},
   "source": [
    "#### Note:  \n",
    "We dont see isolated clusters, in the contrary they are very close together.  \n",
    "We see a clear division in purple leaning right and cyan leaning left, but they are still very much intertwined.   \n",
    "However this is probably from PCA reducing down to only two features, and losing alot of meaning in the process.  \n",
    "Therefore we will continue the machine learning with the full vector.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0606e8-d7f4-4076-84a9-6d6efa3cf97f",
   "metadata": {},
   "source": [
    "### 2.3 Combining data set <a class=\"anchor\" id=\"chapter2.3\"></a>\n",
    "#### Considerations:\n",
    "Usually news stories are clustered on a single day, otherwise there is only 1-2 stories a day, and many days with no stories.\n",
    "Therefore we will make three different combined dataset to be able to compare them with different ML algorithms in the next section.  \n",
    "\n",
    "1. First combined dataset will be based on the news stories as primary. Meaning if there is a news story then the corrosponding stock data from that day will be added. Meaning we will have multiple rows for the same day.\n",
    "2. Second combined dataset will be based on the stock data. Meaning we will combine all information from every news article into a single row based on data. We will discard days with no news stories.\n",
    "3. Third combined dataset will be the same as the second dataset. However we will not discard days with no news stories.\n",
    "\n",
    "Training and splitting will be done individually on each Machine learning section. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc4b0d",
   "metadata": {},
   "source": [
    "#### 2.3.1 First Combined data set <a class=\"anchor\" id=\"chapter2.3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2bfcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvda_df.index=nvda_df['date_time']\n",
    "nvda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f4b28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_ticker_df.index=sorted_ticker_df['date_time']\n",
    "sorted_ticker_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df5e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset_one=pd.merge(nvda_df,sorted_ticker_df, how='inner', left_index=True, right_index=True)\n",
    "combined_dataset_one = combined_dataset_one[['stock', 'title', 'vectors','Low','Open','Volume','High','Close','Adjusted Close','Gain or Loss']]\n",
    "combined_dataset_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449aa4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = combined_dataset_one[\"Gain or Loss\"].value_counts()\n",
    "item_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c540dbb",
   "metadata": {},
   "source": [
    "##### 2.3.1.1 Making a pandas profiling report for the combined data set one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f73aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_one = ProfileReport(combined_dataset_one, title=\"Combined data set one\")\n",
    "profile_one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d3b807",
   "metadata": {},
   "source": [
    "#### 2.3.2 Second Combined data set <a class=\"anchor\" id=\"chapter2.3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b4721",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvda_df.drop(['date_time'], axis = 1, inplace = True) \n",
    "nvda_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6247be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvda_df_temp= nvda_df.groupby('date_time').agg({\"stock\": \"first\", \"vectors\": \"mean\", })\n",
    "nvda_df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c50ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvda_df_count= nvda_df.groupby('date_time').agg({\"vectors\": \"count\", })\n",
    "nvda_df_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b645db",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvda_df_temp['news_count'] = nvda_df_count['vectors']\n",
    "nvda_df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1509c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset_two=pd.merge(nvda_df_temp,sorted_ticker_df, how='inner', left_index=True, right_index=True)\n",
    "combined_dataset_two = combined_dataset_two[['stock', 'vectors','news_count','Low','Open','Volume','High','Close','Adjusted Close','Gain or Loss']]\n",
    "combined_dataset_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c145ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = combined_dataset_two[\"Gain or Loss\"].value_counts()\n",
    "item_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85e70f5",
   "metadata": {},
   "source": [
    "##### 2.3.2.1 Making a profiling report for combined data set two\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d51b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_two = ProfileReport(combined_dataset_two, title=\"Combined data set two\")\n",
    "profile_two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0453a585",
   "metadata": {},
   "source": [
    "#### 2.3.3 Third Combined data set <a class=\"anchor\" id=\"chapter2.3.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee051be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset_three=pd.merge(nvda_df_temp,sorted_ticker_df, how='right', left_index=True, right_index=True)\n",
    "combined_dataset_three = combined_dataset_three[['stock', 'vectors','news_count','Low','Open','Volume','High','Close','Adjusted Close','Gain or Loss']]\n",
    "combined_dataset_three['stock'] = 'NVDA'\n",
    "combined_dataset_three.fillna(0,inplace=True)\n",
    "combined_dataset_three['news_count'] = combined_dataset_three['news_count'].astype(int)\n",
    "combined_dataset_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7521536",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_counts = combined_dataset_three[\"Gain or Loss\"].value_counts()\n",
    "item_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d37e8",
   "metadata": {},
   "source": [
    "##### 2.3.3.1 Making a profiling report for combined data set three\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1537cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile_three = ProfileReport(combined_dataset_three, title=\"Combined data set three\")\n",
    "profile_three"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7df0939",
   "metadata": {},
   "source": [
    "## 3. Machine Learning <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a9d2a",
   "metadata": {},
   "source": [
    "At its heart machine learning is the art of making computers more intelligent without explicitly teaching them how to behave. This is done by identifying patterns in the data. Machine learning derives from artificial intelligence, also known as AI, and is a sub-branch of AI. There are several different types of machine learning and strategies of how to best make use of them. Machine learning involves applying statistics on a dataset to achieve some task. \n",
    "We will be using two out of the three subcategories from the machine learning umbrella. The categories consist of supervised learning, unsupervised learning, and reinforcement learning and we will be using supervised and unsupervised learning.\n",
    "\n",
    "First we will prepare the dataset by splitting into training & test, but also convert into numpy arrays that can be used in ML algorithms.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7934bbd5",
   "metadata": {},
   "source": [
    "### 3.1 Data set preperation for machine learning <a class=\"anchor\" id=\"chapter3.1\"></a>\n",
    "\n",
    "Throughout this section we will prepare three data sets which we will train our models upon. These data sets will be split into train & test sets. Thereafter we will train four different algorithms with our datasets and measure their score or accuracy. Afterwards a confusion matrix will be shown to be able to evaluate our algorithms further. We will also show how we have tweaked our algorithms to perform optimally. Lastly the best performing algorithm, will be saved to a file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd39f407",
   "metadata": {},
   "source": [
    "#### 3.1.1 Data set one preperation <a class=\"anchor\" id=\"chapter3.1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f7c9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the dataset into two arrays\n",
    "y_dataset_one = combined_dataset_one.filter(['Gain or Loss'], axis=1).values\n",
    "#x_dataset_one = combined_dataset_one.filter(['stock', 'title', 'vectors', 'Low', 'Open', 'Volume','High','Close','Adjusted Close'], axis=1).values\n",
    "X_dataset_one = combined_dataset_one.filter(['vectors', 'Low', 'Open', 'Volume','High','Close','Adjusted Close'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1dc65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How is the distribution of data of our labels\n",
    "combined_dataset_one['Gain or Loss'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a87200",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_dataset_one['Gain or Loss'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f72688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our test size will be 20% of the dataset and training will be 80%\n",
    "test_set_size = 0.2\n",
    "# Random_state=3 has been added as a tool to always be able to reproduce the same split \n",
    "random_set_state = 3\n",
    "\n",
    "X_train_dataset_one, X_test_dataset_one, y_train_dataset_one, y_test_dataset_one = train_test_split(X_dataset_one, y_dataset_one, test_size=test_set_size, random_state=random_set_state, stratify=y_dataset_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96c6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dataset_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b1f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_dataset_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d8fc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_train_dataset_one).value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49cf17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_dataset_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc1c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_dataset_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1048bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_test_dataset_one).value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e566a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify that the data has been split correctly.\n",
    "print(\"Training dataset (80%):\", len(X_train_dataset_one))\n",
    "print(\"Test dataset (20%):\", len(X_test_dataset_one))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd30b516",
   "metadata": {},
   "source": [
    "##### 3.1.1.1 Data set one vector isolation in numpy <a class=\"anchor\" id=\"chapter3.1.1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530c1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the vector in a numpy array for machine learning\n",
    "dataset_one_isolated_vectors = X_train_dataset_one[:, 0:1]\n",
    "\n",
    "n = 300\n",
    "X_new = np.empty(shape=[0, n])\n",
    "for x in dataset_one_isolated_vectors:\n",
    "    x_temp = np.array(x.tolist())\n",
    "    X_new = np.append(X_new, x_temp, axis=0)\n",
    "    \n",
    "X_train_dataset_one_np = X_new\n",
    "\n",
    "\n",
    "# Now we do the same for the test set.\n",
    "dataset_one_isolated_vectors = X_test_dataset_one[:, 0:1]\n",
    "\n",
    "n = 300\n",
    "X_new = np.empty(shape=[0, n])\n",
    "for x in dataset_one_isolated_vectors:\n",
    "    x_temp = np.array(x.tolist())\n",
    "    X_new = np.append(X_new, x_temp, axis=0)\n",
    "    \n",
    "X_test_dataset_one_np = X_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd201d0",
   "metadata": {},
   "source": [
    "#### 3.1.2 Data set two preperation <a class=\"anchor\" id=\"chapter3.1.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the dataset into two arrays\n",
    "y_dataset_two = combined_dataset_two.filter(['Gain or Loss'], axis=1).values\n",
    "x_dataset_two = combined_dataset_two.filter(['stock', 'vectors', 'news_count', 'Low', 'Open', 'Volume','High','Close','Adjusted Close'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4ce587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our test size will be 20% of the dataset and training will be 80%\n",
    "test_set_size = 0.2\n",
    "# Random_state=3 has been added as a tool to always be able to reproduce the same split \n",
    "random_set_state = 3\n",
    "\n",
    "X_train_dataset_two, X_test_dataset_two, y_train_dataset_two, y_test_dataset_two = train_test_split(x_dataset_two, y_dataset_two, test_size=test_set_size, random_state=random_set_state, stratify=y_dataset_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b3ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify that the data has been split correctly.\n",
    "print(\"Training dataset (80%):\", len(X_train_dataset_two))\n",
    "print(\"Test dataset (20%):\", len(X_test_dataset_two))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84df917",
   "metadata": {},
   "source": [
    "##### 3.1.2.1 Data set two vector isolation in numpy <a class=\"anchor\" id=\"chapter3.1.2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e06289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the vector in a numpy array for machine learning\n",
    "dataset_two_isolated_vectors = X_train_dataset_two[:, 1:2]\n",
    "dataset_two_isolated_newsCount = X_train_dataset_two[:, 2:3]\n",
    "\n",
    "n = 300\n",
    "X_new = np.empty(shape=[0, n])\n",
    "for x in dataset_two_isolated_vectors:\n",
    "    x_temp = np.array(x.tolist())\n",
    "    X_new = np.append(X_new, x_temp, axis=0)\n",
    "    \n",
    "X_new = np.append(X_new, dataset_two_isolated_newsCount, axis=1)    \n",
    "X_train_dataset_two_np = np.array(X_new.tolist())\n",
    "\n",
    "\n",
    "# Now we do the same for the test set.\n",
    "dataset_two_isolated_vectors = X_test_dataset_two[:, 1:2]\n",
    "dataset_two_isolated_newsCount = X_test_dataset_two[:, 2:3]\n",
    "\n",
    "n = 300\n",
    "X_new = np.empty(shape=[0, n])\n",
    "for x in dataset_two_isolated_vectors:\n",
    "    x_temp = np.array(x.tolist())\n",
    "    X_new = np.append(X_new, x_temp, axis=0)\n",
    "    \n",
    "X_new = np.append(X_new, dataset_two_isolated_newsCount, axis=1)    \n",
    "X_test_dataset_two_np = np.array(X_new.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ad4a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_dataset_two_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db70597",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_dataset_one_np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12153417",
   "metadata": {},
   "source": [
    "#### 3.1.3 Data set three preperation <a class=\"anchor\" id=\"chapter3.1.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb9c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the dataset into two arrays\n",
    "y_dataset_three = combined_dataset_three.filter(['Gain or Loss'], axis=1).values\n",
    "x_dataset_three = combined_dataset_three.filter(['stock', 'vectors', 'news_count', 'Low', 'Open', 'Volume','High','Close','Adjusted Close'], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450ef120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our test size will be 20% of the dataset and training will be 80%\n",
    "test_set_size = 0.2\n",
    "# Random_state=3 has been added as a tool to always be able to reproduce the same split \n",
    "random_set_state = 3\n",
    "\n",
    "X_train_dataset_three, X_test_dataset_three, y_train_dataset_three, y_test_dataset_three = train_test_split(x_dataset_three, y_dataset_three, test_size=test_set_size, random_state=random_set_state, stratify=y_dataset_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ebfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify that the data has been split correctly.\n",
    "print(\"Training dataset (80%):\", len(X_train_dataset_three))\n",
    "print(\"Test dataset (20%):\", len(X_test_dataset_three))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe42a32",
   "metadata": {},
   "source": [
    "##### 3.1.3.1 Data set three vector isolation in numpy <a class=\"anchor\" id=\"chapter3.1.3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b7555f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the vector in a numpy array for machine learning\n",
    "dataset_three_isolated_vectors = X_train_dataset_three[:, 1:2]\n",
    "dataset_three_isolated_newsCount = X_train_dataset_three[:, 2:3]\n",
    "\n",
    "n = 300\n",
    "X_new = np.empty(shape=[0, n])\n",
    "for x in dataset_three_isolated_vectors:\n",
    "    if x != 0:\n",
    "        x_temp = np.array(x.tolist())\n",
    "        X_new = np.append(X_new, x_temp, axis=0)\n",
    "    else:\n",
    "        X_new = np.append(X_new, np.array(np.zeros((1,300))), axis=0)\n",
    "        \n",
    "            \n",
    "X_new = np.append(X_new, dataset_three_isolated_newsCount, axis=1)    \n",
    "X_train_dataset_three_np = np.array(X_new.tolist())\n",
    "\n",
    "# Now we do the same for the test set.\n",
    "dataset_three_isolated_vectors = X_test_dataset_three[:, 1:2]\n",
    "dataset_three_isolated_newsCount = X_test_dataset_three[:, 2:3]\n",
    "\n",
    "n = 300\n",
    "X_new = np.empty(shape=[0, n])\n",
    "for x in dataset_three_isolated_vectors:\n",
    "    if x != 0:\n",
    "        x_temp = np.array(x.tolist())\n",
    "        X_new = np.append(X_new, x_temp, axis=0)\n",
    "    else:\n",
    "        X_new = np.append(X_new, np.array(np.zeros((1,300))), axis=0)\n",
    "    \n",
    "X_new = np.append(X_new, dataset_three_isolated_newsCount, axis=1)    \n",
    "X_test_dataset_three_np = np.array(X_new.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d302c2d6",
   "metadata": {},
   "source": [
    "### 3.2 K-means <a class=\"anchor\" id=\"chapter3.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f68fb5",
   "metadata": {},
   "source": [
    "#### 3.2.1 Theory <a class=\"anchor\" id=\"chapter3.2.1\"></a>\n",
    "\n",
    "K-means is an unsupervised learning method which is used for clustering the data points in a data set.\n",
    "K-means will make the number of clusters as specified by the value of K. Each data point is closer to its own cluster center. The clusters will be made iterative since the algorithm iterates through the data points. Two of the methods that can be used to determine K is the Elbow method which is calculated from the lowest distortion, mean squared error, and the Silhouette method which estimates how close points are to other points in the cluster and how far it is from other clusters. Advantages with K-means can be that it is fairly simple, effective, always gives a solution even though it isn't always a good answer, and it is quick. Disadvantages could be that it only uses numeric data, is sensitive to outliers, difficult to estimate the quality, and only good for compact clusters. Furthermore, depending on how the distance is calculated, chain clustering can happen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75405b7a",
   "metadata": {},
   "source": [
    "#### 3.2.2 Machine Learning data set one <a class=\"anchor\" id=\"chapter3.2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e7a63d",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 3.2.2.1 Training the model <a class=\"anchor\" id=\"chapter3.2.2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0293a737-977b-4e6b-84eb-791d5efc5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First training of our unsupervised learning method will be with a value of 2 beacuse we have 2 labels .\n",
    "kmeans_one = KMeans(n_clusters=2, random_state=42)\n",
    "%time kmeans_one.fit(X_train_dataset_one_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd67bae8",
   "metadata": {},
   "source": [
    "##### 3.2.2.2 Evaluating the model <a class=\"anchor\" id=\"chapter3.2.2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ec249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score with a k value of 2\n",
    "kmeans_one.score(X_test_dataset_one_np, y_test_dataset_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ba450b",
   "metadata": {},
   "source": [
    "##### 3.2.2.3 Estimating the errors in prediction <a class=\"anchor\" id=\"chapter3.2.2.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85375c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = kmeans_one.predict(X_test_dataset_one_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309531f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_one, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_one, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_one, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_one, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bebd88-7b8f-4272-aa79-84ac09685c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_one, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc2e90-95da-4f6d-83cc-de54185a737d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_one, predictions)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45b5243-92e4-45fb-88dd-b7b8883df8cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 3.2.2.4 Second run of K-Means with optimal value of K <a class=\"anchor\" id=\"chapter3.2.2.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d50aa-c4f9-4cbc-b592-df51898aef62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the Model with K-Means\n",
    "# Determine k by minimizing the distortion - \n",
    "# the sum of the squared distances between each observation vector and its centroid\n",
    "distortions = []\n",
    "K = range(2,50)\n",
    "for k in K:\n",
    "    model = KMeans(n_clusters=k).fit(X_test_dataset_one_np)\n",
    "    model.fit(X_test_dataset_one_np)\n",
    "    distortions.append(sum(np.min(cdist(X_test_dataset_one_np, model.cluster_centers_, 'euclidean'), axis=1)) / X_test_dataset_one_np.shape[0]) \n",
    "print(\"Distortion: \", distortions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adcb52d-2d7c-4e05-8459-f50526bd1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the elbow to try and determine an optimal k value\n",
    "plt.title('Elbow Method for optimal K')\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb22146-fde9-40ef-99b1-02a2f3b4117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try and explore this further by clustering our data using K-Means (make sure that there is a good number of clusters)\n",
    "k_range = range(5, 150, 5)\n",
    "kmeans_per_k = []\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(X_train_dataset_one_np)\n",
    "    kmeans_per_k.append(kmeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a60b1af-c1b4-4095-b69c-4950bc86a047",
   "metadata": {},
   "source": [
    "Lets investigate which K gets the best score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cda5de-68ca-4d0d-ba3f-6e34f65c864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the silhouette method to find the best value of K\n",
    "silhouette_scores = [silhouette_score(X_train_dataset_one_np, model.labels_)\n",
    "                     for model in kmeans_per_k]\n",
    "best_index = np.argmax(silhouette_scores)\n",
    "best_k = k_range[best_index]\n",
    "best_score = silhouette_scores[best_index]\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(k_range, silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "plt.plot(best_k, best_score, \"rs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc687da-46f7-4a93-8fed-e922ad7e4f63",
   "metadata": {},
   "source": [
    "Here we see that the the silhouette method implies that our optimal use of k would be 140.\n",
    "Let us try and evaluate our new findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b1831b-97d5-48f1-a12d-88caa6603064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing it with an inertia diagram\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "best_inertia = inertias[best_index]\n",
    "\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(k_range, inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.plot(best_k, best_inertia, \"rs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7ae807-d42d-445d-ae76-735a5eca51ed",
   "metadata": {},
   "source": [
    "From this diagram it can be hard to spot the optimal use of k but the diagram does imply that our optimal use of k again would be 140. \n",
    "We can then draw a conclusion from this result combined with our result from the silhouette method and train our K-means algorithm with \n",
    "a k value of 140."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27851bf0-ed95-498b-8873-9b8bfe259ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best model with the best index (cluster value)\n",
    "best_model = kmeans_per_k[best_index]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c82a92-26d7-4b77-9689-328ffcc6176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_one_opt = KMeans(n_clusters=best_model.n_clusters, random_state=42)\n",
    "%time kmeans_one_opt.fit(X_train_dataset_one_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7bd86d-52af-4185-b074-73e21be4512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "secondRun = kmeans_one_opt.score(X_test_dataset_one_np)\n",
    "print('Second run score:' + str(secondRun))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a75f91e",
   "metadata": {},
   "source": [
    "##### 3.2.2.5 Visualizing The Result <a class=\"anchor\" id=\"chapter3.3.2.6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85fc2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = kmeans_one.cluster_centers_\n",
    "fig = plt.figure()\n",
    "plt.title('Discovered Clusters')\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_test_dataset_one_np[:,0], X_test_dataset_one_np[:,1],  marker='o', cmap='viridis', c=y_test_dataset_one)\n",
    "ax.scatter(cluster_centers[:,0], cluster_centers[:,1], marker='x', \n",
    "           color='red', s=100, linewidth=3, zorder=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bc2d5f",
   "metadata": {},
   "source": [
    "#### 3.2.3 Machine Learning data set two<a class=\"anchor\" id=\"chapter3.2.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e519bf5a",
   "metadata": {},
   "source": [
    "##### 3.2.3.1 Training the model <a class=\"anchor\" id=\"chapter3.2.3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26defad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First training of our unsupervised learning method will be with a value of 2 beacuse we have 2 labels .\n",
    "kmeans_two = KMeans(n_clusters=2, random_state=42)\n",
    "%time kmeans_two.fit(X_train_dataset_two_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223d740",
   "metadata": {},
   "source": [
    "##### 3.2.3.2 Evaluating the model <a class=\"anchor\" id=\"chapter3.2.3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d24af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score with a k value of 2\n",
    "kmeans_two.score(X_test_dataset_two_np, y_test_dataset_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c1f0b2",
   "metadata": {},
   "source": [
    "##### 3.2.3.3 Estimating the errors in prediction <a class=\"anchor\" id=\"chapter3.2.3.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99398be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = kmeans_two.predict(X_test_dataset_two_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c958c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_two, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_two, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_two, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_two, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7060b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_two, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d5d6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_two, predictions)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d438dcfa",
   "metadata": {},
   "source": [
    "##### 3.2.3.4 Second run of K-Means with a more optimal value of K <a class=\"anchor\" id=\"chapter3.2.3.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd566bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the Model with K-Means\n",
    "# Determine k by minimizing the distortion - \n",
    "# the sum of the squared distances between each observation vector and its centroid\n",
    "distortions = []\n",
    "K = range(2,50)\n",
    "for k in K:\n",
    "    model = KMeans(n_clusters=k).fit(X_test_dataset_two_np)\n",
    "    distortions.append(sum(np.min(cdist(X_test_dataset_two_np, model.cluster_centers_, 'euclidean'), axis=1)) / X_test_dataset_two_np.shape[0]) \n",
    "print(\"Distortion: \", distortions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43eb697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the elbow to try and determine an optimal k value\n",
    "plt.title('Elbow Method for optimal K')\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b1229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try and explore this further by clustering our data using K-Means (make sure that there is a good number of clusters)\n",
    "k_range = range(5, 150, 5)\n",
    "kmeans_per_k = []\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(X_train_dataset_two_np)\n",
    "    kmeans_per_k.append(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83ad924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the silhouette method to find the best value of K\n",
    "silhouette_scores = [silhouette_score(X_train_dataset_two_np, model.labels_)\n",
    "                     for model in kmeans_per_k]\n",
    "best_index = np.argmax(silhouette_scores)\n",
    "best_k = k_range[best_index]\n",
    "best_score = silhouette_scores[best_index]\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(k_range, silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "plt.plot(best_k, best_score, \"rs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f119fd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing it with an inertia diagram\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "best_inertia = inertias[best_index]\n",
    "\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(k_range, inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.plot(best_k, best_inertia, \"rs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ddcc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best model with the best index (cluster value)\n",
    "best_model = kmeans_per_k[best_index]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b67d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_two = KMeans(n_clusters=best_model.n_clusters, random_state=42)\n",
    "%time kmeans_two.fit(X_train_dataset_one_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45bc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "secondRun = kmeans_two.score(X_test_dataset_one_np)\n",
    "print('Second run score:' + str(secondRun))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a9dc8b",
   "metadata": {},
   "source": [
    "##### 3.2.3.5 Visualizing The Data <a class=\"anchor\" id=\"chapter3.2.3.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374cb141",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = kmeans_two.cluster_centers_\n",
    "fig = plt.figure()\n",
    "plt.title('Discovered Clusters')\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_test_dataset_two_np[:,0], X_test_dataset_two_np[:,1],  marker='o', cmap='viridis', c=y_test_dataset_two)\n",
    "ax.scatter(cluster_centers[:,0], cluster_centers[:,1], marker='x', \n",
    "           color='red', s=100, linewidth=3, zorder=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec13ea36",
   "metadata": {},
   "source": [
    "#### 3.2.4 Machine Learning data set three<a class=\"anchor\" id=\"chapter3.2.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e16942",
   "metadata": {},
   "source": [
    "##### 3.2.4.1 Training the model <a class=\"anchor\" id=\"chapter3.2.4.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a94c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First training of our unsupervised learning method will be with a value of 2 beacuse we have 2 labels .\n",
    "kmeans_three = KMeans(n_clusters=2, random_state=42)\n",
    "%time kmeans_three.fit(X_train_dataset_three_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bef304",
   "metadata": {},
   "source": [
    "##### 3.2.4.2 Evaluating the model <a class=\"anchor\" id=\"chapter3.2.4.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f610bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Score with a k value of 2\n",
    "kmeans_three.score(X_test_dataset_three_np, y_test_dataset_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bbb6e7",
   "metadata": {},
   "source": [
    "##### 3.2.4.3 Estimating the errors in prediction <a class=\"anchor\" id=\"chapter3.2.4.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc60e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = kmeans_three.predict(X_test_dataset_three_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be7a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_three, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_three, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_three, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_three, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_three, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaf09e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "print(confusion_matrix(y_test_dataset_three, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40109dda",
   "metadata": {},
   "source": [
    "##### 3.2.4.4 Second run of K-Means with a more optimal value of K <a class=\"anchor\" id=\"chapter3.2.4.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b179f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating the Model with K-Means\n",
    "# Determine k by minimizing the distortion - \n",
    "# the sum of the squared distances between each observation vector and its centroid\n",
    "distortions = []\n",
    "K = range(2,50)\n",
    "for k in K:\n",
    "    model = KMeans(n_clusters=k).fit(X_test_dataset_three_np)\n",
    "    distortions.append(sum(np.min(cdist(X_test_dataset_three_np, model.cluster_centers_, 'euclidean'), axis=1)) / X_test_dataset_three_np.shape[0]) \n",
    "print(\"Distortion: \", distortions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb2ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the elbow to try and determine an optimal k value\n",
    "plt.title('Elbow Method for optimal K')\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0884befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try and explore this further by clustering our data using K-Means (make sure that there is a good number of clusters)\n",
    "k_range = range(5, 150, 5)\n",
    "kmeans_per_k = []\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42).fit(X_train_dataset_three_np)\n",
    "    kmeans_per_k.append(kmeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252a891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the silhouette method to find the best value of K\n",
    "silhouette_scores = [silhouette_score(X_train_dataset_three_np, model.labels_)\n",
    "                     for model in kmeans_per_k]\n",
    "best_index = np.argmax(silhouette_scores)\n",
    "best_k = k_range[best_index]\n",
    "best_score = silhouette_scores[best_index]\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(k_range, silhouette_scores, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Silhouette score\", fontsize=14)\n",
    "plt.plot(best_k, best_score, \"rs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011afc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing it with an inertia diagram\n",
    "inertias = [model.inertia_ for model in kmeans_per_k]\n",
    "best_inertia = inertias[best_index]\n",
    "\n",
    "plt.figure(figsize=(8, 3.5))\n",
    "plt.plot(k_range, inertias, \"bo-\")\n",
    "plt.xlabel(\"$k$\", fontsize=14)\n",
    "plt.ylabel(\"Inertia\", fontsize=14)\n",
    "plt.plot(best_k, best_inertia, \"rs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71636ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the best model with the best index (cluster value)\n",
    "best_model = kmeans_per_k[best_index]\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8650f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_one = KMeans(n_clusters=best_model.n_clusters, random_state=42)\n",
    "%time kmeans_one.fit(X_train_dataset_three_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e764cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "secondRun = kmeans_one.score(X_test_dataset_three_np)\n",
    "print('Second run score:' + str(secondRun))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8c019f",
   "metadata": {},
   "source": [
    "##### 3.2.4.5 Visualizing the data <a class=\"anchor\" id=\"chapter3.2.4.5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bab4212",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centers = kmeans_one.cluster_centers_\n",
    "fig = plt.figure()\n",
    "plt.title('Discovered Clusters')\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X_test_dataset_three_np[:,0], X_test_dataset_three_np[:,1],  marker='o', cmap='viridis', c=y_test_dataset_three)\n",
    "ax.scatter(cluster_centers[:,0], cluster_centers[:,1], marker='x', \n",
    "           color='red', s=100, linewidth=3, zorder=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8cdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28632fab-54cd-4472-b9c8-acc91cf3991c",
   "metadata": {},
   "source": [
    "#### 3.2.5 Summary <a class=\"anchor\" id=\"chapter3.2.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86752f2b-ec40-4107-a60b-5983c11db7f7",
   "metadata": {},
   "source": [
    "We can see that on the second run first data set has the largest run score, afterwards it is the third data set and lastly the second data set has the lowest value.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8f30bf",
   "metadata": {},
   "source": [
    "### 3.3 K-Nearest Neighbors <a class=\"anchor\" id=\"chapter3.3\"></a>\n",
    "\n",
    "Through out this section we will try to fit our datasets with the supervised learning method called K-Nearest Neighbors, which is used for classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab694714",
   "metadata": {},
   "source": [
    "#### 3.3.1 Theory <a class=\"anchor\" id=\"chapter3.3.1\"></a>\n",
    "K-Nearest Neighbors is a machine learning algorithm which relies on labeled input data to learn and produce an appropriate output when given new unlabeled data. As it is with supervised machine learning we have to supervise the training of our algorithm. In our case we want to make the algorithm learn whether the stock price (\"Gain or Loss\") goes up (1) or down (0). This is our classification problem and the output is in form of a discrete value 1 or 0. It is standard practice to represent the output (label) of a classification algorithm as an integer and only for representational purpose. Therefor there should not be performed any mathematical operations on this.\n",
    "\n",
    "K-Nearest Neighbors assumes that similar things exist in close proximity. This is done by calculating the distance between points and can be done in form of a straight line which is called the Euclidean distance. K is a value we have to input ourselves and tells the algorithm how many neighbors the algorithm should look at (measure the distance to) and from there decide what label fits the data point. If we increase the number of k the predictions become more stable, but this is only up to a certain point. Eventually there will be an increase in the number of errors and then we know we have chosen a too high k value. To be sure we have a tiebreaker it is a good idea to make k an odd number to be able to perform a majority vote.\n",
    "\n",
    "\n",
    "Advantages with this algorithm is that it is simple and easy to implement. There is no need to tune several parameters or build a model. Lastly this algorithm can be used for classification and regression which makes it versatile.\n",
    "A disadvantage with this algorithm is that it gets significantly slower as the number of predictors (variables/features) increase as well as the data size.\n",
    "A use case for chosing k-nearest neighbor could be to recommend a movie. \n",
    "Example: \"Given our movie dataset, what are the 5 most similar movies to a movie query?\". Which also could be translated to our dataset in form of associating headline news with specific sectors in the stock market."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3034446b",
   "metadata": {},
   "source": [
    "#### 3.3.2 Machine Learning Data set one <a class=\"anchor\" id=\"chapter3.3.2\"></a>\n",
    "Let us begin with training our first dataset and see how accurate K-nearest neighbor can be used to predict our label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c96b7d",
   "metadata": {},
   "source": [
    "##### 3.3.2.1 Training the model <a class=\"anchor\" id=\"chapter3.3.2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd35392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting K for our model to two since we have 2 labels..\n",
    "k = 2\n",
    "# Create an instance of the KNN classification model.\n",
    "knn_model_one = KNeighborsClassifier(n_neighbors=k)\n",
    "knn_model_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17205be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to our train data\n",
    "%time knn_model_one.fit(X_train_dataset_one_np, y_train_dataset_one.ravel())\n",
    "#%time knn_model.fit(X_train_dataset_one, y_train_dataset_one)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd2067e",
   "metadata": {},
   "source": [
    "Tuning the model for data set one to find the best K because before we chose K to be 2 as we have to labels, but we want to see if there is a better K. To this we will make use of GridSEarchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b63680",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new a knn model\n",
    "knn2_one = KNeighborsClassifier()\n",
    "\n",
    "#create a dictionary of all values we want to test for n_neighbors with a numpy array\n",
    "param_grid = {'n_neighbors': np.arange(1, 150)}\n",
    "\n",
    "#use gridsearch to test all values for n_neighbors\n",
    "knn_gscv1 = GridSearchCV(knn2_one, param_grid, cv=5)\n",
    "\n",
    "knn_model_one_opt = knn_gscv1.fit(X_train_dataset_one_np, y_train_dataset_one.ravel())\n",
    "\n",
    "knn_model_one_opt.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14024803",
   "metadata": {},
   "source": [
    "As we can see the best k is 55 and not 2 as we previously assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e624a7b",
   "metadata": {},
   "source": [
    "##### 3.3.2.2 Evaluating the model <a class=\"anchor\" id=\"chapter3.3.2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c840c637",
   "metadata": {},
   "source": [
    "First we look at the score for the knn model where k is 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d07364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the score for this model with k = 2 using Cross validation method for scoring\n",
    "cv_scores = cross_val_score(knn_model_one, X_train_dataset_one_np, y_train_dataset_one.ravel(), cv=5)\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print(\"cv_scores mean:{}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f8374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at the score for this model with k = 2 using the holdout method using test dataset\n",
    "knn_model_one.score(X_test_dataset_one_np, y_test_dataset_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6521e220",
   "metadata": {},
   "source": [
    "Now we look at the score for the optimized k model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the score for this model with k = 53 using Cross validation method for scoring\n",
    "cv_scores = cross_val_score(knn_model_one_opt, X_train_dataset_one_np, y_train_dataset_one.ravel(), cv=5)\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print(\"cv_scores mean:{}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b015933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at the score for this model with optimized K as 53 using the holdout method using test dataset\n",
    "knn_model_one_opt.score(X_test_dataset_one_np, y_test_dataset_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e2017c",
   "metadata": {},
   "source": [
    "The score is higher when with the knn_model_one_opt model compared to the old original one. \n",
    "\n",
    "We will be using the new model for further work as it gives the better score making it the better choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e8fe41",
   "metadata": {},
   "source": [
    "##### 3.3.2.3 Estimating the errors in prediction <a class=\"anchor\" id=\"chapter3.3.2.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab29a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn_model_one_opt.predict(X_test_dataset_one_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656b41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_one, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_one, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_one, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_one, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83057588-c153-4953-8f56-61911848be17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_one, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f1177-e693-405d-a794-85b9777d1183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_one, predictions)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e71ef28",
   "metadata": {},
   "source": [
    "##### 3.3.2.4 Visualizing The Result <a class=\"anchor\" id=\"chapter3.3.2.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cfc99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cd9d95",
   "metadata": {},
   "source": [
    "#### 3.3.3 Machine Learning data set two <a class=\"anchor\" id=\"chapter3.3.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad034da0",
   "metadata": {},
   "source": [
    "##### 3.3.3.1 Training the model <a class=\"anchor\" id=\"chapter3.3.3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8675d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting K to two for our model.\n",
    "k = 2\n",
    "# Create an instance of the KNN classification model.\n",
    "knn_model_two = KNeighborsClassifier(n_neighbors=k)\n",
    "knn_model_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to our train data\n",
    "%time knn_model_two.fit(X_train_dataset_two_np, y_train_dataset_two.ravel())\n",
    "#%time knn_model.fit(X_train_dataset_one, y_train_dataset_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12b6c34",
   "metadata": {},
   "source": [
    "Tuning the model for dataset two by finding the best K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a15f154",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2_two = KNeighborsClassifier()\n",
    "\n",
    "#create a dictionary of all values we want to test for n_neighbors with a numpy array\n",
    "param_grid = {'n_neighbors': np.arange(1, 150)}\n",
    "\n",
    "#use gridsearch to test all values for n_neighbors\n",
    "knn_gscv2 = GridSearchCV(knn2_two, param_grid, cv=5)\n",
    "\n",
    "knn_model_two_opt = knn_gscv2.fit(X_train_dataset_two_np, y_train_dataset_two.ravel())\n",
    "\n",
    "knn_model_two_opt.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abebd98a",
   "metadata": {},
   "source": [
    "We can see that the optimal number to k is 136."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2b4716",
   "metadata": {},
   "source": [
    "##### 3.3.3.2 Evaluating the model <a class=\"anchor\" id=\"chapter3.3.3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd37fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the score for this model with k = 2 using Cross validation method for scoring\n",
    "cv_scores = cross_val_score(knn_model_two, X_train_dataset_two_np, y_train_dataset_two.ravel(), cv=5)\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print(\"cv_scores mean:{}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b946d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at the score for this model with k = 2\n",
    "knn_model_two.score(X_test_dataset_two_np, y_test_dataset_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e336b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the score for this model with k = 53 using Cross validation method for scoring\n",
    "cv_scores = cross_val_score(knn_model_two_opt, X_train_dataset_two_np, y_train_dataset_two.ravel(), cv=5)\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print(\"cv_scores mean:{}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f1b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We look at the score for this model with optimized K as 136\n",
    "knn_model_two_opt.score(X_test_dataset_two_np, y_test_dataset_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca18fd3a",
   "metadata": {},
   "source": [
    "#WRONG: REWRITE.. \n",
    "We can see that even though the our search has led us to believe that the best parameter for k is 136, this is not the case here as the old model gives us a better score. this is probably because K is so large that it becomes inaccurate on the smaller test dataset. Therefore, we will continue to use the old model for further work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6fd36f",
   "metadata": {},
   "source": [
    "##### 3.3.3.3 Estimating the errors in prediction <a class=\"anchor\" id=\"chapter3.3.3.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c2895",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn_model_two_opt.predict(X_test_dataset_two_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1a5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_two, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_two, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_two, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_two, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab693b-7ef4-48a2-a156-02137f84dde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_two, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a6887-ffef-49dc-a451-1c3caf4e232d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_two, predictions)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27797c5f",
   "metadata": {},
   "source": [
    "##### 3.3.3.4 Visualizing The Results<a class=\"anchor\" id=\"chapter3.3.3.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162162ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a96ca6f",
   "metadata": {},
   "source": [
    "#### 3.3.4 Machine Learning data set three <a class=\"anchor\" id=\"chapter3.3.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823f414",
   "metadata": {},
   "source": [
    "##### 3.3.4.1 Training the model <a class=\"anchor\" id=\"chapter3.3.4.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9fba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting K for our model to 2 since we have 2 labels.\n",
    "k = 2\n",
    "# Create an instance of the KNN classification model.\n",
    "knn_model_three = KNeighborsClassifier(n_neighbors=k)\n",
    "knn_model_three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ffb51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to our train data\n",
    "%time knn_model_three.fit(X_train_dataset_three_np, y_train_dataset_three.ravel())\n",
    "#%time knn_model.fit(X_train_dataset_one, y_train_dataset_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8726a34b",
   "metadata": {},
   "source": [
    "Tuning our knn model for data set three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160f873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2_three = KNeighborsClassifier()\n",
    "\n",
    "#create a dictionary of all values we want to test for n_neighbors with a numpy array\n",
    "param_grid = {'n_neighbors': np.arange(1, 150)}\n",
    "\n",
    "#use gridsearch to test all values for n_neighbors\n",
    "knn_gscv3 = GridSearchCV(knn2_three, param_grid, cv=5)\n",
    "\n",
    "knn_model_three_opt = knn_gscv3.fit(X_train_dataset_three_np, y_train_dataset_three.ravel())\n",
    "\n",
    "knn_model_three_opt.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4611b",
   "metadata": {},
   "source": [
    "The best k is believed to be 11 and not what we initially thought it to be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b950cb7a",
   "metadata": {},
   "source": [
    "##### 3.3.4.2 Evaluating the model <a class=\"anchor\" id=\"chapter3.3.4.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c118aa4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the score for this model with k = 2 using Cross validation method for scoring\n",
    "cv_scores = cross_val_score(knn_model_three, X_train_dataset_three_np, y_train_dataset_three.ravel(), cv=5)\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print(\"cv_scores mean:{}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15aa5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the score for this model\n",
    "knn_model_three.score(X_test_dataset_three_np, y_test_dataset_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5eb7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the score for this model with k = 53 using Cross validation method for scoring\n",
    "cv_scores = cross_val_score(knn_model_three_opt, X_train_dataset_three_np, y_train_dataset_three.ravel(), cv=5)\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print(\"cv_scores mean:{}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da01b58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model_three_opt.score(X_test_dataset_three_np, y_test_dataset_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202f622f",
   "metadata": {},
   "source": [
    "Here we can see tht the new and improved model has a slightly higher score, which is why we will be using it in the next few steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f652ea1",
   "metadata": {},
   "source": [
    "##### 3.3.4.3 Estimating the errors in prediction <a class=\"anchor\" id=\"chapter3.3.4.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c23de81",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = knn_model_three_opt.predict(X_test_dataset_three_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20804a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_three, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_three, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_three, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_three, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76996583-1894-40b0-806d-6fc8bacd20c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_three, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab37f6-5553-424c-ace0-6c4d3e1f6d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_three, predictions)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0a3179",
   "metadata": {},
   "source": [
    "##### 3.3.4.4 Visualizing The Results <a class=\"anchor\" id=\"chapter3.3.4.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0291468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc761a4",
   "metadata": {},
   "source": [
    "#### 3.3.5 Summary <a class=\"anchor\" id=\"chapter3.3.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b307b682",
   "metadata": {},
   "source": [
    "Jamming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e6e1f1",
   "metadata": {},
   "source": [
    "### 3.4 Gaussian Naive Bayes <a class=\"anchor\" id=\"chapter3.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514fa01b",
   "metadata": {},
   "source": [
    "#### 3.4.1 Theory <a class=\"anchor\" id=\"chapter3.4.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c7e248",
   "metadata": {},
   "source": [
    "Naive Bayes is an easy classification method and it has proven to perform well when dealing with a large amount of data, which in our case could be beneficial since we are working with the stock market and the news flow attached to this. Naive Bayes has shown to especially be effective when having to perform as a spam filter, text classifier, and recommender system. Naive Bayes makes predictions about labels using the Bayes theory of probability which in terms makes the model a probabilistic classification model. Bayes theorem is a formula which uses conditional probability of an event, A happening given another event B has previously happened.  \n",
    "The formula:   \n",
    "P(A|B) = P(B|A) * P(A) / P(B)   \n",
    "P(A|B) = the probability of event A happening given event B has happened.   \n",
    "P(B|A) = the probability of event B happening given event A has happened.   \n",
    "P(A) = the probability of event A.   \n",
    "P(B) = the probability of event B.   \n",
    "\n",
    "As a classifier Naive Bayes has the following characteristics:\n",
    "- Naive Bayes makes the assumption that the predictors contribute equally and independently to predict the output (label). \n",
    "- Though the model makes an assumption about the predictors being independent of each other it is not what would resemble circumstances in the real world. However, the outcome is satisfactory in the majority of cases and this also makes the model computational fast. If the model did not make this assumption it would take a long time to compute which would not be beneficial in something like a spam filter, where there is a need to classify spam rather fast, so the users do not have to wait to receive their emails. \n",
    "- Naive Bayes is often used to categorize text because the dimensionality of the data often is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0c5fc5",
   "metadata": {},
   "source": [
    "#### 3.4.2 Machine Learning data set one <a class=\"anchor\" id=\"chapter3.4.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d154593",
   "metadata": {},
   "source": [
    "##### 3.4.2.1 Training the model <a class=\"anchor\" id=\"chapter3.4.2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5377972",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_one = GaussianNB()\n",
    "gnb_one.fit(X_train_dataset_one_np, y_train_dataset_one.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f2a39f",
   "metadata": {},
   "source": [
    "##### 3.4.2.2 Evaluating the model <a class=\"anchor\" id=\"chapter3.4.2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cf5cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the score for this model using Cross validation method for scoring\n",
    "cv_scores = cross_val_score(gnb_one, X_train_dataset_one_np, y_train_dataset_one.ravel(), cv=5)\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print(\"cv_scores mean:{}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b18f51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the score for this model\n",
    "gnb_one.score(X_test_dataset_one_np, y_test_dataset_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3209ea",
   "metadata": {},
   "source": [
    "##### 3.4.2.3 Estimating the errors in prediction <a class=\"anchor\" id=\"chapter3.4.2.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39049d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gnb_one.predict(X_test_dataset_one_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b739f2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_one, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_one, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_one, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_one, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef82fa61-dde0-4cd0-ad47-175ff865a638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_one, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f387ff-9da5-4bcc-b549-354a5690b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_one, predictions)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a88f2a0",
   "metadata": {},
   "source": [
    "##### 3.4.2.4 Visualizing The Result <a class=\"anchor\" id=\"chapter3.4.2.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165d7340",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df05b3",
   "metadata": {},
   "source": [
    "#### 3.4.3 Machine Learning Data set two <a class=\"anchor\" id=\"chapter3.4.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4784d67",
   "metadata": {},
   "source": [
    "##### 3.4.3.1 Training the model <a class=\"anchor\" id=\"chapter3.4.3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc82ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_two = GaussianNB()\n",
    "gnb_two.fit(X_train_dataset_two_np, y_train_dataset_two.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e62109f",
   "metadata": {},
   "source": [
    "##### 3.4.3.2 Evaluating the model <a class=\"anchor\" id=\"chapter3.4.3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32614142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the score for this model using Cross validation method for scoring\n",
    "cv_scores = cross_val_score(gnb_two, X_train_dataset_two_np, y_train_dataset_two.ravel(), cv=5)\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print(\"cv_scores mean:{}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the score for this model\n",
    "gnb_two.score(X_test_dataset_two_np, y_test_dataset_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea49ed8",
   "metadata": {},
   "source": [
    "##### 3.4.3.3 Estimating the errors in prediction <a class=\"anchor\" id=\"chapter3.4.3.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e76871",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gnb_two.predict(X_test_dataset_two_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c2314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_two, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_two, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_two, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_two, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762e4ecc-e626-4718-93d4-170d60c41300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_two, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe482b2f-b92d-4639-9156-5a31174a8efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_two, predictions)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f2a420",
   "metadata": {},
   "source": [
    "##### 3.4.3.4 Visualizing The Result <a class=\"anchor\" id=\"chapter3.4.3.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3b0566",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d094183",
   "metadata": {},
   "source": [
    "#### 3.4.4 Machine Learning data set three <a class=\"anchor\" id=\"chapter3.4.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a107cd",
   "metadata": {},
   "source": [
    "##### 3.4.4.1 Training the model <a class=\"anchor\" id=\"chapter3.4.4.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbef640e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_three = GaussianNB()\n",
    "gnb_three.fit(X_train_dataset_three_np, y_train_dataset_three.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a552e110",
   "metadata": {},
   "source": [
    "##### 3.4.4.2 Evaluating the model <a class=\"anchor\" id=\"chapter3.4.4.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd27c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the score for this model using Cross validation method for scoring\n",
    "cv_scores = cross_val_score(gnb_three, X_train_dataset_three_np, y_train_dataset_three.ravel(), cv=5)\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print(\"cv_scores mean:{}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01460e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the score for this model\n",
    "gnb_three.score(X_test_dataset_three_np, y_test_dataset_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf09ae7e",
   "metadata": {},
   "source": [
    "##### 3.4.4.3 Estimating the errors in prediction <a class=\"anchor\" id=\"chapter3.4.4.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = gnb_three.predict(X_test_dataset_three_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486b638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_three, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_three, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_three, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_three, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f29a56-6613-4d82-ad2c-b88ecb075ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_three, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43920ad-7211-4f2c-9cac-52e185a6a24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_three, predictions)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8e9010",
   "metadata": {},
   "source": [
    "##### 3.4.4.4 Visualizing data<a class=\"anchor\" id=\"chapter3.4.4.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990c61dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6795841a",
   "metadata": {},
   "source": [
    "#### 3.4.5 Summary <a class=\"anchor\" id=\"chapter3.4.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6664981d",
   "metadata": {},
   "source": [
    "### 3.5 Decision Tree <a class=\"anchor\" id=\"chapter3.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01befda1",
   "metadata": {},
   "source": [
    "#### 3.5.1 Theory <a class=\"anchor\" id=\"chapter3.5.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7ad6ee",
   "metadata": {},
   "source": [
    "Our two above shown algorithms are part of the supervised learning method called classification which the following algorithm decision tree also is a part of. Imagine having a set of conditions on a set of nodes that run from top to bottom. These nodes could contain simple question with conditions (yes/no), which would lead to another node. At the end of a branch when it does not split anymore is where a decision would be found. The decision is what will be classified as a label. In the simplest form this is how a decision tree works and this methodology is known as learning decision tree and is called a classification tree. This form of algorithm \"grows\" a tree in the background deciding on which features to pick and what conditions to use for branching (splitting) and finally knowing when to stop. In a procedure like this all features are considered, and different split points are tried and tested with a cost function. A split is evaluated and chosen by the lowest cost which is also the best cost. Just like K-Nearest Neighbors a decision tree can be used for classification and regression which makes it versatile, but the preferred use is for solving classification problems. In short, a decision tree contains two kinds of nodes, a decision node and a leaf node. From a decision node there will be made a decision which can contain several branches. A leaf node derives from the decision node and is the output from the decision that has been made along the way. Furthermore, leaf nodes do not contain further branches. Reasons to choose the decision tree algorithm could be that the algorithm usually mimics human thinking while making a decision which can make the algorithm easy to understand. Furthermore, the logic behind a decision tree can be shown in a tree-like structure which again contributes to an easy to understand understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49fecb",
   "metadata": {},
   "source": [
    "#### 3.5.2 Machine Learning data set one <a class=\"anchor\" id=\"chapter3.5.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633c9f3a",
   "metadata": {},
   "source": [
    "##### 3.5.2.1 Training the model <a class=\"anchor\" id=\"chapter3.5.2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e07c5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_one = DecisionTreeClassifier()\n",
    "dtc_one.fit(X_train_dataset_one_np, y_train_dataset_one.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8c4842",
   "metadata": {},
   "source": [
    "##### 3.5.2.2 Evaluating the model <a class=\"anchor\" id=\"chapter3.5.2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a568d772",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the score for this model using Cross validation method for scoring\n",
    "cv_scores = cross_val_score(dtc_one, X_train_dataset_one_np, y_train_dataset_one.ravel(), cv=5)\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print(\"cv_scores mean:{}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859d4dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_one.score(X_test_dataset_one_np, y_test_dataset_one)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764ba1d",
   "metadata": {},
   "source": [
    "##### 3.5.2.3 Estimating the errors in prediction <a class=\"anchor\" id=\"chapter3.5.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6841a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dtc_one.predict(X_test_dataset_one_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989f0e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_one, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_one, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_one, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_one, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288dca1b-8e19-41ce-86c2-2774b7099b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_one, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93176ef-0033-4d17-a49c-25d70a9c4c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_one, predictions)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca693e3",
   "metadata": {},
   "source": [
    "##### 3.5.2.4 Visualizing data (?) <a class=\"anchor\" id=\"chapter3.5.2.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f8015-b3b0-40d1-9420-0a1f8bf34517",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18dd97",
   "metadata": {},
   "source": [
    "#### 3.5.3 Machine Learning data set two <a class=\"anchor\" id=\"chapter3.5.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f08b8ea",
   "metadata": {},
   "source": [
    "##### 3.5.3.1 Training the model <a class=\"anchor\" id=\"chapter3.5.3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e53415",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_two = DecisionTreeClassifier()\n",
    "dtc_two.fit(X_train_dataset_two_np, y_train_dataset_two.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc945e5b",
   "metadata": {},
   "source": [
    "##### 3.5.3.2 Evaluating the model <a class=\"anchor\" id=\"chapter3.5.3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b966ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the score for this model using Cross validation method for scoring\n",
    "cv_scores = cross_val_score(dtc_two, X_train_dataset_two_np, y_train_dataset_two.ravel(), cv=5)\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print(\"cv_scores mean:{}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8235f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_two.score(X_test_dataset_two_np, y_test_dataset_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b44dc94",
   "metadata": {},
   "source": [
    "##### 3.5.3.3 Estimating the errors in prediction <a class=\"anchor\" id=\"chapter3.5.3.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca7dbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dtc_two.predict(X_test_dataset_two_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c38749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_two, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_two, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_two, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_two, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ffff6b-75a1-46e9-9fc5-b51950511f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_two, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ed130-0a70-4c16-9fdb-be329563a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_two, predictions)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36699856",
   "metadata": {},
   "source": [
    "##### 3.5.3.4 Visualizing The Result <a class=\"anchor\" id=\"chapter3.5.3.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494a0ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e603857",
   "metadata": {},
   "source": [
    "#### 3.5.4 Machine Learning data set three <a class=\"anchor\" id=\"chapter3.5.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0376633",
   "metadata": {},
   "source": [
    "##### 3.5.4.1 Training the model <a class=\"anchor\" id=\"chapter3.5.4.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacf7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_three = DecisionTreeClassifier()\n",
    "dtc_three.fit(X_train_dataset_three_np, y_train_dataset_three.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89564a4",
   "metadata": {},
   "source": [
    "##### 3.5.4.2 Evaluating the model <a class=\"anchor\" id=\"chapter3.5.4.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e2dbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We look at the score for this model using Cross validation method for scoring\n",
    "cv_scores = cross_val_score(dtc_three, X_train_dataset_three_np, y_train_dataset_three.ravel(), cv=5)\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print(\"cv_scores mean:{}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb529a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtc_three.score(X_test_dataset_three_np, y_test_dataset_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c957a3f9",
   "metadata": {},
   "source": [
    "##### 3.5.4.3 Estimating the errors in prediction <a class=\"anchor\" id=\"chapter3.5.4.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cab95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = dtc_three.predict(X_test_dataset_three_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a97299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_three, predictions)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_three, predictions)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_three, predictions)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_three, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b65065-ceef-4879-b742-ec4e2c50cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_three, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcd3596-b12c-4dda-afd2-2d06c86d79ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_three, predictions)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfe73b6",
   "metadata": {},
   "source": [
    "##### 3.5.4.4 Visualizing The Result<a class=\"anchor\" id=\"chapter3.5.4.4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='winter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18eaee0",
   "metadata": {},
   "source": [
    "#### 3.5.5 Summary <a class=\"anchor\" id=\"chapter3.5.5\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec96981-6220-44be-b31f-143e4e4d6782",
   "metadata": {},
   "source": [
    "# blah blah"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b573d3",
   "metadata": {},
   "source": [
    "### 3.6 Prediction Performance Comparator\n",
    "This will be our comparision result by not using machine learning and just using random selection on the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e98275",
   "metadata": {},
   "source": [
    "#### 3.6.1 Dataset One"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5104db54",
   "metadata": {},
   "source": [
    "#### 3.6.1.1 All Zero Prediction Performance for dataset one <a class=\"anchor\" id=\"chapter3.6.1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa23da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "allzeroes = np.zeros((612, 1), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5541f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_one, allzeroes)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_one, allzeroes, zero_division=True)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_one, allzeroes, zero_division=True)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_one, allzeroes, zero_division=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6001dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_one, allzeroes, zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81a86e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_one, allzeroes)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52145347",
   "metadata": {},
   "source": [
    "#### 3.6.1.2 All One Prediction Performance for dataset one <a class=\"anchor\" id=\"chapter3.6.1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5abc3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "allones = np.ones((612, 1), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3109b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_one, allones)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_one, allones)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_one, allones)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_one, allones)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5725c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_one, allones, zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a62d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_one, allones)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e041c76",
   "metadata": {},
   "source": [
    "#### 3.6.1.3 Random Prediction Performance for dataset one <a class=\"anchor\" id=\"chapter3.6.1.1\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e63a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for lp in range(10000):\n",
    "    randomArray = np.random.randint(0,2,(612, 1)) \n",
    "    scores.append(float(accuracy_score(y_test_dataset_one, randomArray)))\n",
    "print(\"cv_scores mean:{}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b022a8c9",
   "metadata": {},
   "source": [
    "#### 3.6.2 Dataset Two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2b947",
   "metadata": {},
   "source": [
    "#### 3.6.2.1 All Zero Prediction Performance for dataset two <a class=\"anchor\" id=\"chapter3.6.2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c42d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "allzeroes = np.zeros((225, 1), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_two, allzeroes)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_two, allzeroes, zero_division=True)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_two, allzeroes, zero_division=True)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_two, allzeroes, zero_division=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396bf5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_two, allzeroes, zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_two, allzeroes)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cab59a",
   "metadata": {},
   "source": [
    "#### 3.6.2.2 All Ones Prediction Performance for dataset two <a class=\"anchor\" id=\"chapter3.6.2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "allones = np.ones((225, 1), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc115325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_two, allones)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_two, allones)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_two, allones)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_two, allones)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab738f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_two, allones, zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469af6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_two, allones)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7eac3a",
   "metadata": {},
   "source": [
    "#### 3.6.2.3 Random Prediction Performance for dataset two<a class=\"anchor\" id=\"chapter3.6.2.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58365d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for lp in range(10000):\n",
    "    randomArray = np.random.randint(0,2,(225, 1)) \n",
    "    scores.append(float(accuracy_score(y_test_dataset_two, randomArray)))\n",
    "print(\"cv_scores mean:{}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab67e9dc",
   "metadata": {},
   "source": [
    "#### 3.6.3 Dataset Three"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2159bc3b",
   "metadata": {},
   "source": [
    "#### 3.6.3.1 All Zero Prediction Performance for dataset three <a class=\"anchor\" id=\"chapter3.6.2.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8224aa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "allzeroes = np.zeros((591, 1), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9921de3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_three, allzeroes)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_three, allzeroes, zero_division=True)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_three, allzeroes, zero_division=True)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_three, allzeroes, zero_division=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103b46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_three, allzeroes, zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d72dde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_three, allzeroes)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fe8b8",
   "metadata": {},
   "source": [
    "#### 3.6.3.2 All Ones Prediction Performance for dataset three <a class=\"anchor\" id=\"chapter3.6.2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe4c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "allones = np.ones((591, 1), dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa44168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the accuracy of the predictions\n",
    "print('Accuracy score: ', format(accuracy_score(y_test_dataset_three, allones)))\n",
    "print('Precision score: ', format(precision_score(y_test_dataset_three, allones)))\n",
    "print('Recall score: ', format(recall_score(y_test_dataset_three, allones)))\n",
    "print('F1 score: ', format(f1_score(y_test_dataset_three, allones)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b29108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report provides a breakdown of each class by precision, recall, f1-score and support\n",
    "print(classification_report(y_test_dataset_three, allones, zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7877d1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix provides an indication of the the errors of prediction\n",
    "cf_matrix = confusion_matrix(y_test_dataset_three, allones)\n",
    "print(cf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc6a448",
   "metadata": {},
   "source": [
    "#### 3.6.3.3 Random Prediction Performance for dataset three<a class=\"anchor\" id=\"chapter3.6.2.3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35a121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for lp in range(10000):\n",
    "    randomArray = np.random.randint(0,2,(591, 1)) \n",
    "    scores.append(float(accuracy_score(y_test_dataset_three, randomArray)))\n",
    "print(\"cv_scores mean:{}\".format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36f82ee",
   "metadata": {},
   "source": [
    "#### 3.6.4 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dadde64",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.7 Final Summary<a class=\"anchor\" id=\"chapter3.7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c6572",
   "metadata": {},
   "source": [
    "Since Gaussian Naive Bayes was which was trained on data set one, was the best performing candidate it is the one that will be saved as our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c751066",
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(gnb_one, '../model/model.pkl')\n",
    "#msmodel = joblib.load('../deploy/msmodel.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10f77b8",
   "metadata": {},
   "source": [
    "## 4. Visualization <a class=\"anchor\" id=\"chapter4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a92dc-3b73-4dd4-bfa8-720aeeb8d8d0",
   "metadata": {},
   "source": [
    "Throughout this section we will be implementing two candlestick charts which will be build upon our own data set one. This is since we find this data set to represent our data in the best format because it includes date/time, title which is news, volume, and stock ticker. Furthermore, we have chosen to show how the end product could be visualized with the most recent data pulled from Yahoo finance. The meaning with these candlestick charts is to give an end user the possibility to view the trend of the stock, volume, and the news attached to that day to the specific stock. We will be focussing on the technology stock NVIDIA (ticker: NVDA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2436ae09-d7fe-4f27-99f1-be6f82ab38a1",
   "metadata": {},
   "source": [
    "#### 4.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030cfd0-2a7f-48c5-ade9-168379a047c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charting library\n",
    "!pip install plotly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d4d1bb-8056-4f62-b3b9-5bd738e24e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historical market data from Yahoo Finance\n",
    "!pip install yfinance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b218c-e396-435a-9f1d-7e5e7926856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The chosen dataset: combined_dataset_one\n",
    "combined_dataset_one.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79436ad-629a-494f-99a2-baf77149f1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 trading session days with news\n",
    "# 253 trading days in a year\n",
    "unique_days = combined_dataset_one.index[-737:].unique()\n",
    "unique_days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c011971f-7cdc-4f94-aaf5-7dfe581c6b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last wo years data gathered from yahoo finance\n",
    "nvda = yfinance.Ticker('NVDA')\n",
    "hist = nvda.history(period='2y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85591626-edf3-4530-b66f-f30f194add4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure that we have the correct data\n",
    "hist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a94faaf-ec15-4c14-bfc9-7525fce05d73",
   "metadata": {},
   "source": [
    "#### 4.2 Charting with our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544cdcf9-b330-48b9-96f7-964a3472868a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Scatter(x=unique_days,y=combined_dataset_one['Close'][-737:].unique(), mode='lines'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c93f9-831e-4ed5-ab4c-aca0ebb80c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dots and lines showcase\n",
    "fig = go.Figure(data=go.Scatter(x=unique_days,y=combined_dataset_one['Close'][-737:].unique(), mode='lines+markers'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4691e5bc-6ee4-4362-8d82-b297580512f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot for volume on same chart\n",
    "\n",
    "fig2 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig2.add_trace(go.Scatter(x=unique_days,y=combined_dataset_one['Close'][-737:].unique(),name='Price'),secondary_y=False)\n",
    "fig2.add_trace(go.Bar(x=unique_days,y=hist['Volume'][-737:],name='Volume'),secondary_y=True)\n",
    "fig2.update_yaxes(range=[0,1000000000],secondary_y=True)\n",
    "fig2.update_yaxes(visible=False, secondary_y=True)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97470051-c8d1-4759-b932-8700faff1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candlestick chart\n",
    "fig3 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig3 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig3.add_trace(go.Candlestick(x=unique_days,\n",
    "                              open=combined_dataset_one['Open'][-737:].unique(),\n",
    "                              high=combined_dataset_one['High'][-737:].unique(),\n",
    "                              low=combined_dataset_one['Low'][-737:].unique(),\n",
    "                              close=combined_dataset_one['Close'][-737:].unique(),\n",
    "                              name='Price'\n",
    "                             ))\n",
    "fig3.add_trace(go.Bar(x=unique_days,y=combined_dataset_one['Volume'][-737:].unique(),name='Volume'),secondary_y=True)\n",
    "fig3.update_yaxes(range=[0,1000000000],secondary_y=True)\n",
    "fig3.update_yaxes(visible=False, secondary_y=True)\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7da467-fcaf-4870-8ac1-034725608cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making column to see whether a stock is green or red on the day\n",
    "combined_dataset_one['diff'] = combined_dataset_one['Close'] - combined_dataset_one['Open']\n",
    "combined_dataset_one.loc[combined_dataset_one['diff']>=0, 'color'] = 'green'\n",
    "combined_dataset_one.loc[combined_dataset_one['diff']<0, 'color'] = 'red'\n",
    "combined_dataset_one.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4945a1c5-900b-41ca-a943-e89f7b591349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the shape of our data\n",
    "combined_dataset_one.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80368e80-5d46-4c9b-a9c8-f98b9e3e76ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving averages\n",
    "combined_dataset_one['Close'].rolling(20).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812a34e-d6c5-460d-af4b-059898fb3a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do we have news attached to our title column\n",
    "combined_dataset_one['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f5f2a6-411f-4da6-bde1-8538605fd08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together\n",
    "fig4 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig4.add_trace(go.Candlestick(x=unique_days,\n",
    "                              hovertext=combined_dataset_one['title'][-737:],\n",
    "                              open=combined_dataset_one['Open'][-737:].unique(),\n",
    "                              high=combined_dataset_one['High'][-737:].unique(),\n",
    "                              low=combined_dataset_one['Low'][-737:].unique(),\n",
    "                              close=combined_dataset_one['Close'][-737:].unique(),\n",
    "                              name='Price'))\n",
    "fig4.add_trace(go.Bar(x=unique_days, y=combined_dataset_one['Volume'][-737:].unique(), name='Volume', marker={'color':combined_dataset_one['color']}),secondary_y=True)\n",
    "fig4.update_yaxes(range=[0,700000000],secondary_y=True)\n",
    "fig4.update_yaxes(visible=False, secondary_y=True)\n",
    "fig4.update_layout(xaxis_rangeslider_visible=False)  #hide range slider\n",
    "fig4.update_layout(title={'text':'NVDA Our Combined Dataset', 'x':0.5})\n",
    "fig4.update_xaxes(rangebreaks = [\n",
    "                       dict(bounds=['sat','mon']), # hide weekends\n",
    "                       #dict(bounds=[16, 9.5], pattern='hour'), # for hourly chart, hide non-trading hours (24hr format)\n",
    "                       dict(values=[\"2019-01-01\",\"2020-06-30\"]) #hide Christmas and New Year\n",
    "                                ])\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dffc1c6-1ae5-4785-bae6-e95cf9e14b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the interactive candlestick chart to be view and used in html format\n",
    "fig4.write_html(r'C:\\Users\\jplm\\Desktop\\Softwareudvikler\\Data science\\Afleveringer\\Eksamen\\visualization\\charts\\GraphOurDataSet.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e96569-6695-4a4f-a0e5-a12ccae86093",
   "metadata": {},
   "source": [
    "#### 4.3 Charting with Yahoo Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24c1a76-8bac-4672-afcf-8d3a2b2453d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvda = yfinance.Ticker('NVDA')\n",
    "hist = nvda.history(period='2y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef8abe-cba5-4915-b2bd-a10971395115",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig4 = go.Figure(data=go.Scatter(x=hist.index,y=hist['Close'], mode='lines'))\n",
    "fig4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ba4ef-6e93-4b7c-a21b-3c8b4109dd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dots and lines\n",
    "fig5 = go.Figure(data=go.Scatter(x=hist.index,y=hist['Close'], mode='lines+markers'))\n",
    "fig5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91853ce0-a1e9-4e6c-907a-906f935b5932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subplot for volume on same chart\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig6 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig6.add_trace(go.Scatter(x=hist.index,y=hist['Close'],name='Price'),secondary_y=False)\n",
    "fig6.add_trace(go.Bar(x=hist.index,y=hist['Volume'],name='Volume'),secondary_y=True)\n",
    "fig6.update_yaxes(range=[0,1000000000],secondary_y=True)\n",
    "fig6.update_yaxes(visible=False, secondary_y=True)\n",
    "fig6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052d8e90-6e75-4a68-992c-a6e22236577d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Candlestick chart\n",
    "fig7 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig7 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig7.add_trace(go.Candlestick(x=hist.index,\n",
    "                              open=hist['Open'],\n",
    "                              high=hist['High'],\n",
    "                              low=hist['Low'],\n",
    "                              close=hist['Close'],\n",
    "                             ))\n",
    "fig7.add_trace(go.Bar(x=hist.index,y=hist['Volume'],name='Volume'),secondary_y=True)\n",
    "fig7.update_yaxes(range=[0,1000000000],secondary_y=True)\n",
    "fig7.update_yaxes(visible=False, secondary_y=True)\n",
    "fig7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af1c472-85d8-4152-8c11-807138d6dbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making column to see whether a stock is green or red on the day\n",
    "hist['diff'] = hist['Close'] - hist['Open']\n",
    "hist.loc[hist['diff']>=0, 'color'] = 'green'\n",
    "hist.loc[hist['diff']<0, 'color'] = 'red'\n",
    "hist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77840eb-962e-40ee-a228-0f29ecdea643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving averages\n",
    "hist['Close'].rolling(20).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ecb1d3-d5cc-496b-8adb-2e5f5e5adced",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist['Close'].rolling(50).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699e49af-ab7e-4cda-83f0-f19628147afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist['Close'].rolling(200).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff2337e-ae61-4624-bcb0-b5f2101c092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Putting it all together\n",
    "fig8 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "fig8.add_trace(go.Candlestick(x=hist.index,\n",
    "                              open=hist['Open'],\n",
    "                              high=hist['High'],\n",
    "                              low=hist['Low'],\n",
    "                              close=hist['Close'],\n",
    "                              name='Price'))\n",
    "fig8.add_trace(go.Scatter(x=hist.index,y=hist['Close'].rolling(window=20).mean(),marker_color='yellow',name='20 Day MA'))\n",
    "fig8.add_trace(go.Scatter(x=hist.index,y=hist['Close'].rolling(window=50).mean(),marker_color='blue',name='50 Day MA'))\n",
    "fig8.add_trace(go.Scatter(x=hist.index,y=hist['Close'].rolling(window=200).mean(),marker_color='orange',name='200 Day MA'))\n",
    "fig8.add_trace(go.Bar(x=hist.index, y=hist['Volume'], name='Volume', marker={'color':hist['color']}),secondary_y=True)\n",
    "fig8.add_trace(go.Scatter(x=hist.index,y=hist['Volume'].rolling(window=50).mean(),marker_color='black',name='Volume MA'), secondary_y=True) \n",
    "fig8.update_yaxes(range=[0,700000000],secondary_y=True)\n",
    "fig8.update_yaxes(visible=False, secondary_y=True)\n",
    "fig8.update_layout(xaxis_rangeslider_visible=False)  #hide range slider\n",
    "fig8.update_layout(title={'text':'NVDA Yahoo Finance Data', 'x':0.5})\n",
    "fig8.update_xaxes(rangebreaks = [\n",
    "                       dict(bounds=['sat','mon']), # hide weekends\n",
    "                       #dict(bounds=[16, 9.5], pattern='hour'), # for hourly chart, hide non-trading hours (24hr format)\n",
    "                       dict(values=[\"2021-12-25\",\"2022-01-01\"]) #hide Christmas and New Year\n",
    "                                ])\n",
    "fig8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea370feb-6ddd-476a-a5df-f5549bbe3f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the interactive candlestick chart to be view and used in html format\n",
    "fig8.write_html(r'C:\\Users\\jplm\\Desktop\\Softwareudvikler\\Data science\\Afleveringer\\Eksamen\\visualization\\charts\\GraphYahooFinance.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e9de52-8980-42ff-b1dd-2ab2a4d42e34",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 5. Deployment of solution <a class=\"anchor\" id=\"chapter5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee4c700-5555-4f3c-b842-da3477702bff",
   "metadata": {},
   "source": [
    "Throughout this section we will be loading our stored model to be able to use it on our web server. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537791f8-af1a-448a-b4b4-1b4ca44c7811",
   "metadata": {},
   "source": [
    "#### 5.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350bbfd-45cf-4342-8882-d820849096c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For serialization and deserialization of data from/to file\n",
    "msmodel = joblib.load('../model/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f8f516-630d-4eb1-bd1a-4490088d3b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing if we can read from the file\n",
    "test_prediction = msmodel.predict(y_test_dataset_one)\n",
    "test_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f46b5af-b741-4ce5-ac3d-f8acd6ebe214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing the server\n",
    "!pip install flask-bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9608916c-a611-4aed-9b48-dab65195221d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piping fields from the html file\n",
    "!pip install wtforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac23921-df69-4d53-b1ec-42e1e82311f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile mswebapp.py \n",
    "# Making an executable\n",
    "\n",
    "# Imports\n",
    "import joblib\n",
    "import pickle\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "\n",
    "\n",
    "# Create an instance (our app)\n",
    "#app = Flask(__name__)\n",
    "app = Flask(__name__, template_folder='../templates/')\n",
    "\n",
    "msmodel = joblib.load('../model/model.pkl')\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "\n",
    "@app.route('/hi/<name>')\n",
    "def hello(name = None):\n",
    "    return render_template('start.html', name=name)\n",
    "# name is parameter in the template: {{name}}\n",
    "\n",
    "@app.route('/predict')\n",
    "def predict():\n",
    "    return render_template('prediction.html')\n",
    "\n",
    "@app.route('/predicted', methods=['GET', 'POST'])\n",
    "def predicted():\n",
    "    if request.method == 'POST':\n",
    "        x1 = request.form['x1']\n",
    "        x2 = request.form['x2']\n",
    "        X = [[x1, x2]]\n",
    "        predicted = msmodel.predict(X)\n",
    "          \n",
    "        return render_template(\"predicted.html\", content=X, prediction=predicted)\n",
    "    \n",
    "@app.route('/bye')\n",
    "def bye():\n",
    "    return render_template('bye.html')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f391116-40b2-4594-bf25-d26cf4d0286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the web server\n",
    "!python mswebapp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5773e65",
   "metadata": {},
   "source": [
    "## 6. Project Conclusion <a class=\"anchor\" id=\"chapter6\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae393d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f5a8b-d288-47f4-b6ef-29be9d6050a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527b856a-cd3d-4360-915a-abf0a5f966dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
